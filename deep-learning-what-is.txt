3:I[9261,[],""]
5:I[6675,[],""]
6:I[1441,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","918","static/chunks/918-55e4a914e8809569.js","462","static/chunks/462-12ae486a1be74331.js","423","static/chunks/423-1777a1736af94f26.js","185","static/chunks/app/layout-ae468821186f76f4.js"],""]
7:I[8390,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","918","static/chunks/918-55e4a914e8809569.js","462","static/chunks/462-12ae486a1be74331.js","423","static/chunks/423-1777a1736af94f26.js","185","static/chunks/app/layout-ae468821186f76f4.js"],"ThemeProvider"]
8:I[7083,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","918","static/chunks/918-55e4a914e8809569.js","462","static/chunks/462-12ae486a1be74331.js","423","static/chunks/423-1777a1736af94f26.js","185","static/chunks/app/layout-ae468821186f76f4.js"],"Header"]
9:I[9605,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-55e21886c07a752a.js"],""]
a:I[3760,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","918","static/chunks/918-55e4a914e8809569.js","462","static/chunks/462-12ae486a1be74331.js","423","static/chunks/423-1777a1736af94f26.js","185","static/chunks/app/layout-ae468821186f76f4.js"],"Toaster"]
b:I[4743,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","918","static/chunks/918-55e4a914e8809569.js","462","static/chunks/462-12ae486a1be74331.js","423","static/chunks/423-1777a1736af94f26.js","185","static/chunks/app/layout-ae468821186f76f4.js"],"GoogleTagManager"]
4:["slug","deep-learning-what-is","d"]
0:["qGSDEiurmeK7XNQ8nRN4G",[[["",{"children":["(blog)",{"children":[["slug","deep-learning-what-is","d"],{"children":["__PAGE__?{\"slug\":\"deep-learning-what-is\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["(blog)",{"children":[["slug","deep-learning-what-is","d"],{"children":["__PAGE__",{},["$L1","$L2",null]]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","(blog)","children","$4","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","(blog)","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},[null,["$","html",null,{"lang":"ko","className":"h-full scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","$L6",null,{"id":"adsbygoogle-init","strategy":"beforeInteractive","src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1154659137489563","crossOrigin":"anonymous"}]}],["$","body",null,{"className":"flex min-h-screen flex-col font-pretendard","children":[["$","$L7",null,{"children":[["$","$L8",null,{}],["$","main",null,{"className":"mt-[calc(64px+env(safe-area-inset-top))] flex flex-1 flex-col","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"grid flex-1 place-content-center text-center","children":[["$","h1",null,{"className":"mb-4 text-2xl font-bold","children":"Not Found"}],["$","p",null,{"className":"mb-8 text-lg","children":"찾을 수 없는 페이지입니다."}],["$","$L9",null,{"href":"/","children":"홈으로","className":"inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 bg-primary text-primary-foreground hover:bg-primary/90 h-10 py-2 mx-auto w-fit px-10"}]]}],"notFoundStyles":[],"styles":null}]}],["$","footer",null,{"className":"mb-16 mt-20 flex flex-col items-center justify-center gap-4 text-center print:hidden","children":[["$","div",null,{"className":"flex justify-center gap-4","children":[["$","$L9",null,{"href":"https://github.com/","target":"_blank","children":["$","svg",null,{"fill":"currentColor","viewBox":"0 0 16 16","height":30,"width":30,"className":"fill-foreground transition hover:fill-pink-600","children":["$","path",null,{"d":"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0016 8c0-4.42-3.58-8-8-8z"}]}]}],["$","$L9",null,{"href":"https://www.linkedin.com/in/","target":"_blank","children":["$","svg",null,{"viewBox":"0 0 24 24","fill":"currentColor","height":30,"width":30,"className":"fill-foreground transition hover:fill-pink-600","children":["$","path",null,{"d":"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"}]}]}]]}],["$","div",null,{"children":["© 2024. ",["$","span",null,{"className":"font-semibold","children":"Devtimes Blog"}]," all rights reserved."]}]]}]]}],["$","$La",null,{}],["$","$Lb",null,{"gtmId":"GTM-N7LWQNXW"}]]}]]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/81c2358a4e4f0309.css","precedence":"next","crossOrigin":""}]],"$Lc"]]]]
d:I[2465,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-55e21886c07a752a.js"],""]
f:I[3574,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-55e21886c07a752a.js"],""]
10:I[938,["605","static/chunks/605-9393b79bc12886be.js","734","static/chunks/734-fab1a4b8e5b2b407.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-55e21886c07a752a.js"],""]
2:["$","div",null,{"className":"prose mx-auto w-full max-w-[750px] px-5 dark:prose-invert sm:px-6","children":[["$","header",null,{"className":"mt-14 text-center","children":[["$","h1",null,{"className":"mb-5 text-3xl","children":"딥러닝 - 딥러닝의 이해"}],["$","div",null,{"className":"mb-3 text-base","children":["$","$L9",null,{"href":"/ai","className":"font-semibold text-pink-600 no-underline underline-offset-4 hover:underline","children":"Ai"}]}],["$","div",null,{"className":"flex justify-center gap-3 text-sm text-gray-500 dark:text-gray-400","children":[["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar-days w-3.5","children":[["$","path","1cmpym",{"d":"M8 2v4"}],["$","path","4m81vk",{"d":"M16 2v4"}],["$","rect","1hopcy",{"width":"18","height":"18","x":"3","y":"4","rx":"2"}],["$","path","8toen8",{"d":"M3 10h18"}],["$","path","6423bh",{"d":"M8 14h.01"}],["$","path","1etili",{"d":"M12 14h.01"}],["$","path","1gbofw",{"d":"M16 14h.01"}],["$","path","lrp35t",{"d":"M8 18h.01"}],["$","path","mhygvu",{"d":"M12 18h.01"}],["$","path","kzsmim",{"d":"M16 18h.01"}],"$undefined"]}],["$","span",null,{"children":"2019년 05월 16일"}]]}],["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-clock3 w-3.5","children":[["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],["$","polyline","1aq6pp",{"points":"12 6 12 12 16.5 12"}],"$undefined"]}],["$","span",null,{"children":[13,"분"]}]]}]]}],["$","hr",null,{"className":"mt-5"}]]}],null,["$","article",null,{"className":"relative","children":[["$","$Ld",null,{"toc":[]}],"$Le"]}],["$","hr",null,{}],["$","$Lf",null,{}],["$","$L10",null,{}]]}]
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1, viewport-fit=cover"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"딥러닝 - 딥러닝의 이해 | DevTimes Blog"}],["$","meta","3",{"name":"description","content":"딥러닝의 개념에 대해 이해해보자."}],["$","meta","4",{"property":"og:title","content":"딥러닝 - 딥러닝의 이해 | DevTimes Blog"}],["$","meta","5",{"property":"og:description","content":"딥러닝의 개념에 대해 이해해보자."}],["$","meta","6",{"property":"og:url","content":"https://blog.devtimes.com/deep-learning-what-is"}],["$","meta","7",{"property":"og:image","content":"https://blog.devtimes.com/posts/ai/deep-learning-what-is/deep_main.png"}],["$","meta","8",{"property":"og:type","content":"article"}],["$","meta","9",{"property":"article:published_time","content":"2019-05-16T00:00:00.000Z"}],["$","meta","10",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","11",{"name":"twitter:title","content":"딥러닝 - 딥러닝의 이해 | DevTimes Blog"}],["$","meta","12",{"name":"twitter:description","content":"딥러닝의 개념에 대해 이해해보자."}],["$","meta","13",{"name":"twitter:image","content":"https://blog.devtimes.com/posts/ai/deep-learning-what-is/deep_main.png"}],["$","link","14",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"518x518"}],["$","link","15",{"rel":"icon","href":"/icon.png?8d4a794a4e2a4e0b","type":"image/png","sizes":"518x518"}],["$","link","16",{"rel":"apple-touch-icon","href":"/apple-icon.png?8d4a794a4e2a4e0b","type":"image/png","sizes":"518x518"}]]
1:null
e:[["$","h1",null,{"id":"개념","children":"개념"}],"\n",["$","p",null,{"children":["$","code",null,{"children":"심층 신경망을 이용한 머신러닝 기법"}]}],"\n",["$","p",null,{"children":"딥러닝이라는 용어는 2006년 캐나다 토론토 대학교의 제프리 힌튼(Geoffrey Hinton) 교수의 논문을 통해 처음 사용이 되었다. 하지만 딥러닝은 사실 새로운 개념이 아니다. 오래전부터 있어오던 인공신경망(Artificial Neural Network, ANN)과 크게 다를 바 없다. '인공신경망'이라고 하면 복잡한 뇌 구조를 생각하면서 어렵게 생각이 들겠지만 실제 뉴런의 행동을 살펴보면 어떻게 이렇게 단순한 행동으로 이루어질까 싶을 정도로 심플하다."}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/neuron.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]]}],"\n",["$","p",null,{"children":"입력신호가 오면 길이에 따른 가중치(weight)를 부여하고 임계값(bias)을 더한 후 축색돌기를 통해 이동한다. 그리고 그 값이 조건에 만족한다면 다음 뉴런으로 넘어가고 그렇지 않다면 넘어가지 않게 된다. 이러한 실제 뉴런에서 이뤄지는 과정을 그대로 수학적으로 구현한 것이다."}],"\n",["$","p",null,{"children":"컴퓨터가 사진 속에서 고양이를 검출해내야 한다고 생각해보자. '고양이'라는 추상적 이미지는 아마 선, 면, 형상, 색깔, 크기 등 다양한 요소들이 조합된 결과물일 것이다. 이것은 아마 '선 30cm 이상은 고양이, 이하는 고양이 아님', 또는 '갈색은 고양이, 빨간색은 고양이 아님' 처럼 간단한 선형 구분으로는 식별해 낼 수 없는 문제이다. 딥러닝은 이 과제를 선 긋고 왜곡하고 합하고를 반복하며 복잡한 공간 속에서의 최적의 구분선을 만들어 내는 목적을 가지고 있다."}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/deep_01.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]]}],"\n",["$","p",null,{"children":"파란선과 빨간선의 영역을 구분한다고 생각해보자. 그냥 구분 선을 긋는다면 아마 왼쪽처럼 불완전하게 그을 수 있을 것이다. 하지만 공간을 왜곡하면 오른쪽 같이 아름답게 구분선을 그릴 수 있다. 이처럼 인공신경망은 선 긋고, 구기고, 합하고를 반복하여 데이터를 처리한다."}],"\n",["$","p",null,{"children":"그럼 어떠한 규칙으로 선을 긋고 공간을 왜곡할까? 바로 데이터에 근거하는 것이다. 일단 대충 선을 긋고 그것들을 살살 움직여가며 구분 결과가 더 좋게 나오도록 선을 움직이는 것인데 이러한 과정을 최적화(optimization)이라고 한다. 딥러닝은 아주 많은 데이터와 아주 오랜 시간의 최적화를 통해 데이터를 학습하게 된다. (양에는 장사 없다고나 할까...)"}],"\n",["$","p",null,{"children":[[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/deep_layer.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]],["$","br",null,{}],"\n","딥러닝은 하나의 뉴런이 아닌 수천 수억개의 뉴런들이 연결되어 이뤄진다. 즉 인공 신경 또한 수많은 연결들을 통해 망을 이룬다. 크게 입력층(input layer)과 출력층(output layer) 그리고 그 사이에 있는 layer들을 묶어서 은닉층(hidden layer)이라고 한다. 위 그림에서는 모든 노드가 연결되어있지만, 꼭 다 연결되어있어야 하는 것은 아니다. 단지 내가 어떻게 신경망을 구성하냐에 따라 정해지는 것이다."]}],"\n",["$","p",null,{"children":["저렇게 망이 이뤄져 있는 것은 알겠는데, 대체 어떻게 학습을 한다는건지 이해가 안 될 수 있다.",["$","br",null,{}],"\n","내가 고양이 사진을 입력에 넣었다면 출력에는 고양이라는 결과가 나와야 맞는 것이다. 고양이가 맞다면 그냥 넘어가면 되지만, 고양이가 아니라고 결과가 나왔다면 이것은 문제가 있는 것이다. 따라서 다시 뒤로 돌아가면서 각 인공 신경의 가중치(weight)와 임계값(bias) 값을 수정하게 된다. 이러한 과정을 반복하며 weight와 bias 값을 조정하고 고양이라는 결과가 나오도록 '학습' 하는 것이다."]}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/image_net.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]]}],"\n",["$","p",null,{"children":"2012년 이미지넷에서 이미지 분류 대회가 있었다. 1000개의 카테고리와 100만개의 이미지로 구성되어 분류 정확도를 겨루는 대회이고, 아래 사진만 보더라도 딥러닝이 이미지 인식 부문에서 얼마나 큰 영향을 끼치고 있는지 알 수 있다. 이 대회 이전에는 기계의 이미지 인식률이 75%를 넘지 못했고 80% 이상의 인식률은 불가능이라는 인식이 있었다. 하지만 이 대회에서 힌튼 교수의 제자 알렉스가 알렉스넷(AlexNet)이라는 딥러닝 기반 알고리즘으로 84.7%를 찍었고, 이후로는 대부분의 참가자들이 딥러닝으로 방향을 돌렸다. 현재는 오류율이 5% 이하의 정확도로 인간의 인식수준을 뛰어넘었다."}],"\n",["$","p",null,{"children":"딥러닝이 대단한 것은 일반적인 기계 학습과 달리 특징 추출(feature extraction)이 자동적으로 이루어지는 점 이다. 기존에는 효과적인 특징을 추출하기 위해 관련 분야 전문가가 오랜 시간동안 직접 특징을 추출하는 수식이나 방법을 고안해야 했다. 이 방법은 개발, 평가 및 보완에 많은 시간이 걸리는데 딥러닝은 이런 과정을 컴퓨터가 대신 하도록 알고리즘을 짠 것으로, 사람에 비해 훨씬 빠르고 효과적으로 수행해도록 학습시켜준다."}],"\n",["$","h1",null,{"id":"발전과정","children":"발전과정"}],"\n",["$","p",null,{"children":"딥러닝의 발전과정을 보면 아주 오래전부터 시작이 되었고 지금에 이르기까지 많은 이들의 노력이 있었다."}],"\n",["$","p",null,{"children":["$","code",null,{"children":"1957 : 최초 신경망 모델 Perceptron 등장"}]}],"\n",["$","p",null,{"children":"퍼셉트론은 인간의 두뇌 움직임을 수학적으로 구성하였다."}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/deep_02.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]]}],"\n",["$","p",null,{"children":["$","code",null,{"children":"1957 ~ 1986 : 첫 빙하기(30년)"}]}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/deep_03.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]]}],"\n",["$","p",null,{"children":["그 당시 AND와 OR의 경우 퍼셉트론을 통한 선형 분리(linearly separable)가 가능했다. 따라서 AND / OR 연산이 가능하게 훈련한 퍼셉트론을 보고 많은 사람들은 기계의 학습 가능성에 대해 큰 기대를 하게 되었다.",["$","br",null,{}],"\n","하지만 이러한 그들의 엄청난 기대에 찬물을 끼얹어버린 것이 XOR 연산에 대한 불가능이었다. 당시 하나의 인공 신경. 즉 퍼셉트론으로는 선형 분리가 불가능해 XOR 연산에 대한 학습이 불가능했다. "]}],"\n",["$","p",null,{"children":["1969년 이 문제를 해결하기 위해 퍼셉트론을 다중으로 겹치면 이 문제를 해결할 수 있음을 증명한 ",["$","strong",null,{"children":"다중 계층 퍼셉트론이 등장"}],"하지만 레이어가 복잡해질수록 연산이 복잡해져서 현실적으로 파라미터값을 구하는 것이 불가능 하였다."]}],"\n",["$","p",null,{"children":["$","code",null,{"children":"1986 : 새로운 학습 방법 등장"}]}],"\n",["$","p",null,{"children":["데이터가 모델을 스스로 찾아내는 역전파(Backpropagation) 등장",["$","br",null,{}],"\n","즉 앞의 진행방향에서 고쳐가는 것이 아니라 결과를 보고 뒤로 가면서 weight와 bias를 조정하는 방법을 고안 하였고, XOR 뿐 아니라 좀 더 복잡한 과정도 해결할 수 있음을 보이며 다시 인공 신경망은 사람들의 관심을 끌기 시작했다."]}],"\n",["$","p",null,{"children":["$","code",null,{"children":"1990 : BOOM"}]}],"\n",["$","p",null,{"children":[[["$","img",null,{"src":"/posts/ai/deep-learning-what-is/deep_04.png","alt":"deep-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"deep-1"}]],["$","br",null,{}],"\n","(터미네이터2 심판의 날 (1991) 대사 中)"]}],"\n",["$","p",null,{"children":"영화 중반부에 터미네이터인 아놀드의 대사 중에 \"My CPU is a neural-net processor.\" 라는 말이 나온다. 스스로 학습할 수 있는 기계를 neural-net processor 라는 말로 설명했고 이 당시에도 neural network에 관심이 있었음을 알 수 있다. "}],"\n",["$","p",null,{"children":[["$","code",null,{"children":"1990 ~ 2000 : 두번째 빙하기(10년)"}],["$","br",null,{}],"\n","신경망이 깊어질수록 원하는 결과를 얻을 수 없다. 오히려 성능이 저하되는 경우가 발생."]}],"\n",["$","p",null,{"children":"다음의 문제를 해결하는데 10년이 걸리게 된다."}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":"Overfitting (과하거나)"}],"\n",["$","li",null,{"children":"Vanishing Gradient (덜하거나)"}],"\n",["$","li",null,{"children":"Too slow  (느리거나)"}],"\n"]}],"\n",["$","p",null,{"children":"이런 문제를 해결하기 위해 연구하는 10년 동안 GUP를 활용한 연산시도, 알고리즘의 발전이 있게 된다.(SVM, Random Forest 등장)"}],"\n",["$","p",null,{"children":["$","code",null,{"children":"2000 : 3가지 한계점의 해결방안 등장"}]}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["과한 적합 해결","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Droupout"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["덜한 적합 해결","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"ReLU(Rectified Linear Unit) : 수렴속도가 시그모이드류 함수 대비 6배 빠름"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["느린 적합 해결","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Adam : 확룰적 경사 하강법(SGD)에서 더 나아가 학습속도와 운동량을 고려한 옵티마이저가 등장"}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":["$","code",null,{"children":"2006 : 드디어 Deep Network. Deep Learning 용어가 사용되기 시작"}]}],"\n",["$","p",null,{"children":"이러한 여러번의 실패로 인해 이후 나온 논문에서는 neural net이라는 단어를 찾을 수 없었고 neural 대신 deep 이라는 단어를 사용했다. 이 당시 neural network 라는 말만 들어가면 논문에서 거절당한다는 말이 있을 정도로 신경망이 외면받던 시기였기 때문에 좀 더 사람들의 이목을 끌 수 있는 단어를 택했던 것 같다."}],"\n",["$","h1",null,{"id":"마치며","children":"마치며"}],"\n",["$","p",null,{"children":"딥 러닝이 화두가 되고 있는 것은 비교적 최근의 일이지만, 딥 러닝의 기본 구조인 인공 신경망(Artificial Neural Network)의 역사는 오래되었다. 1957년에 등장한 초기 신경망인 퍼셉트론(Perceptron)으로 시작해 최근에 쓰이고 있는 발전된 신경망 RNN, LSTM에 대해서도 좀더 자세히 알아보면 좋을 것 같다."}]]
