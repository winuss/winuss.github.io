3:I[9261,[],""]
5:I[6675,[],""]
6:I[8390,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"ThemeProvider"]
7:I[7083,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"Header"]
8:I[9605,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js"],""]
9:I[3760,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"Toaster"]
a:I[3542,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"Analytics"]
b:I[6693,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"SpeedInsights"]
c:I[450,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"GoogleAnalytics"]
d:I[8806,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","229","static/chunks/229-1ed242868e4072e6.js","323","static/chunks/323-71aa8ed227954704.js","637","static/chunks/637-db3f4f84dff1e805.js","185","static/chunks/app/layout-f6bf49daae8218b1.js"],"GoogleTagManager"]
4:["slug","nlp-basic","d"]
0:["ok9b4Xw0HRS6qEaGEMqHa",[[["",{"children":["(blog)",{"children":[["slug","nlp-basic","d"],{"children":["__PAGE__?{\"slug\":\"nlp-basic\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["(blog)",{"children":[["slug","nlp-basic","d"],{"children":["__PAGE__",{},["$L1","$L2",null]]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","(blog)","children","$4","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","(blog)","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},[null,["$","html",null,{"lang":"en","className":"h-full scroll-my-20 scroll-smooth","suppressHydrationWarning":true,"children":["$","body",null,{"className":"flex min-h-screen flex-col font-pretendard","children":[["$","$L6",null,{"children":[["$","$L7",null,{}],["$","main",null,{"className":"mt-[64px] flex flex-1 flex-col","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"grid flex-1 place-content-center text-center","children":[["$","h1",null,{"className":"mb-4 text-2xl font-bold","children":"Not Found"}],["$","p",null,{"className":"mb-8 text-lg","children":"찾을 수 없는 페이지입니다."}],["$","$L8",null,{"href":"/","children":"홈으로","className":"inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 bg-primary text-primary-foreground hover:bg-primary/90 h-10 py-2 mx-auto w-fit px-10"}]]}],"notFoundStyles":[],"styles":null}]}],["$","footer",null,{"className":"mb-16 mt-20 flex flex-col items-center justify-center gap-4 text-center print:hidden","children":[["$","div",null,{"className":"flex justify-center gap-4","children":[["$","$L8",null,{"href":"https://github.com/","target":"_blank","children":["$","svg",null,{"fill":"currentColor","viewBox":"0 0 16 16","height":30,"width":30,"className":"fill-foreground transition hover:fill-pink-600","children":["$","path",null,{"d":"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0016 8c0-4.42-3.58-8-8-8z"}]}]}],["$","$L8",null,{"href":"https://www.linkedin.com/in/","target":"_blank","children":["$","svg",null,{"viewBox":"0 0 24 24","fill":"currentColor","height":30,"width":30,"className":"fill-foreground transition hover:fill-pink-600","children":["$","path",null,{"d":"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"}]}]}]]}],["$","div",null,{"children":["© 2024. ",["$","span",null,{"className":"font-semibold","children":"Devtimes"}]," all rights reserved."]}]]}]]}],["$","$L9",null,{}],["$","$La",null,{}],["$","$Lb",null,{}],["$","$Lc",null,{"gaId":"G-TRBVGE9TYP"}],["$","$Ld",null,{"gtmId":"G-TRBVGE9TYP"}]]}]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/814e03573ee35ccf.css","precedence":"next","crossOrigin":""}]],"$Le"]]]]
f:I[2465,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js"],""]
11:I[3574,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js"],""]
12:I[938,["605","static/chunks/605-9393b79bc12886be.js","614","static/chunks/614-02ab0d044823ada6.js","206","static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js"],""]
2:["$","div",null,{"className":"prose mx-auto w-full max-w-[750px] px-5 dark:prose-invert sm:px-6","children":[["$","header",null,{"className":"mt-14 text-center","children":[["$","h1",null,{"className":"mb-5 text-3xl","children":"NLP - Bag of words, n-gram"}],["$","div",null,{"className":"mb-3 text-base","children":["$","$L8",null,{"href":"/ai","className":"font-semibold text-pink-600 no-underline underline-offset-4 hover:underline","children":"Ai"}]}],["$","div",null,{"className":"flex justify-center gap-3 text-sm text-gray-500 dark:text-gray-400","children":[["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar-days w-3.5","children":[["$","path","1cmpym",{"d":"M8 2v4"}],["$","path","4m81vk",{"d":"M16 2v4"}],["$","rect","1hopcy",{"width":"18","height":"18","x":"3","y":"4","rx":"2"}],["$","path","8toen8",{"d":"M3 10h18"}],["$","path","6423bh",{"d":"M8 14h.01"}],["$","path","1etili",{"d":"M12 14h.01"}],["$","path","1gbofw",{"d":"M16 14h.01"}],["$","path","lrp35t",{"d":"M8 18h.01"}],["$","path","mhygvu",{"d":"M12 18h.01"}],["$","path","kzsmim",{"d":"M16 18h.01"}],"$undefined"]}],["$","span",null,{"children":"2019년 08월 07일"}]]}],["$","div",null,{"className":"flex items-center gap-1","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-clock3 w-3.5","children":[["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],["$","polyline","1aq6pp",{"points":"12 6 12 12 16.5 12"}],"$undefined"]}],["$","span",null,{"children":[9,"분"]}]]}]]}],["$","hr",null,{"className":"mt-5"}]]}],null,["$","article",null,{"className":"relative","children":[["$","$Lf",null,{"toc":[]}],"$L10"]}],["$","hr",null,{}],["$","$L11",null,{}],["$","$L12",null,{}]]}]
e:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"NLP - Bag of words, n-gram | DevTimes"}],["$","meta","3",{"name":"description","content":"NLP - Bag of words, n-gram"}],["$","meta","4",{"property":"og:title","content":"NLP - Bag of words, n-gram | DevTimes"}],["$","meta","5",{"property":"og:description","content":"NLP - Bag of words, n-gram"}],["$","meta","6",{"property":"og:url","content":"https://devtimes.com/nlp-basic"}],["$","meta","7",{"property":"og:image","content":"https://devtimes.com/posts/ai/nlp-basic/cover.png"}],["$","meta","8",{"property":"og:type","content":"article"}],["$","meta","9",{"property":"article:published_time","content":"2019-08-07T00:00:00.000Z"}],["$","meta","10",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","11",{"name":"twitter:title","content":"NLP - Bag of words, n-gram | DevTimes"}],["$","meta","12",{"name":"twitter:description","content":"NLP - Bag of words, n-gram"}],["$","meta","13",{"name":"twitter:image","content":"https://devtimes.com/posts/ai/nlp-basic/cover.png"}],["$","link","14",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"518x518"}],["$","link","15",{"rel":"icon","href":"/icon.png?8d4a794a4e2a4e0b","type":"image/png","sizes":"518x518"}],["$","link","16",{"rel":"apple-touch-icon","href":"/apple-icon.png?8d4a794a4e2a4e0b","type":"image/png","sizes":"518x518"}]]
1:null
10:[["$","p",null,{"children":"자연어 처리(natural language processing)는 인간의 언어 현상을 기계적으로 분석해서 컴퓨터가 이해할 수 있는 형태로 만드는 자연 언어 이해 혹은 그러한 형태를 다시 인간이 이해할 수 있는 언어로 표현하는 제반 기술을 의미한다. (위키피디아)"}],"\n",["$","p",null,{"children":"간단하게 말하면, 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일 이라고 생각하면 될 것 같다."}],"\n",["$","h1",null,{"id":"텍스트","children":"텍스트"}],"\n",["$","p",null,{"children":"기계학습 모델을 만들기 위해서는 데이터를 모델에 맞게 변형시켜 주어야 한다. 알고리즘에서 텍스트를 그대로 받아들일수 없기 때문에 받아들일 수 있는 어떤 숫자값으로 변환을 해주어야 한다. 하지만 텍스트는 일단 언어가 제각기 다르기 떄문에 텍스트 자체를 어떻게 숫자화 할지 부터 시작해야한다."}],"\n",["$","p",null,{"children":"그럼 어떤 방법들이 있는지 살펴보자."}],"\n",["$","h1",null,{"id":"bowbag-of-words","children":"BOW(bag of words)"}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/nlp-basic/bow-01.png","alt":"nlp-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"nlp-1"}]]}],"\n",["$","p",null,{"children":"BOW(Bag of words)는 텍스트 데이터를 표현하는 방법 중 하나로 가장 간단하지만 효과적이라 기계학습에서 널리 쓰이는 방법이다. BOW는 텍스트의 구조와 상관없이 단어들을 담는 가방(Bag)으로 생각하면 된다."}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"The game is fun\nThe game is interesting\nThe game is not funny\n"}]}],"\n",["$","p",null,{"children":"세 문장에서 나타나는 단아들을 모으고 세 문장을 각각 binary vector로 표현하는 것이 BOW이다. 각각의 문장을 binary vector로 표현하면 다음과 같다."}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"the"}],["$","th",null,{"children":"game"}],["$","th",null,{"children":"is"}],["$","th",null,{"children":"fun"}],["$","th",null,{"children":"interesting"}],["$","th",null,{"children":"not"}],["$","th",null,{"children":"funny"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"0"}],["$","td",null,{"children":"0"}],["$","td",null,{"children":"0"}]]}]}]]}],"\n",["$","div",null,{"className":"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary","children":[false,["$","div",null,{"className":"callout-contents flex-1","children":["$undefined",["\n",["$","p",null,{"children":["The game is fun : ",["$","strong",null,{"children":"[1, 1, 1, 1, 0, 0, 0]"}]]}],"\n"]]}]]}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"the"}],["$","th",null,{"children":"game"}],["$","th",null,{"children":"is"}],["$","th",null,{"children":"fun"}],["$","th",null,{"children":"interesting"}],["$","th",null,{"children":"not"}],["$","th",null,{"children":"funny"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"0"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"0"}],["$","td",null,{"children":"0"}]]}]}]]}],"\n",["$","div",null,{"className":"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary","children":[false,["$","div",null,{"className":"callout-contents flex-1","children":["$undefined",["\n",["$","p",null,{"children":["The game is interesting : ",["$","strong",null,{"children":"[1, 1, 1, 0, 1, 0, 0]"}]]}],"\n"]]}]]}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"the"}],["$","th",null,{"children":"game"}],["$","th",null,{"children":"is"}],["$","th",null,{"children":"fun"}],["$","th",null,{"children":"interesting"}],["$","th",null,{"children":"not"}],["$","th",null,{"children":"funny"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"0"}],["$","td",null,{"children":"0"}],["$","td",null,{"children":"1"}],["$","td",null,{"children":"1"}]]}]}]]}],"\n",["$","div",null,{"className":"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary","children":[false,["$","div",null,{"className":"callout-contents flex-1","children":["$undefined",["\n",["$","p",null,{"children":["The game is not funny : ",["$","strong",null,{"children":"[1, 1, 1, 0, 0, 1, 1]"}]]}],"\n"]]}]]}],"\n",["$","p",null,{"children":"만약 이 가방에 들어있지 않는 단어가 포함된 문장이 있어도 BOW는 그 없는 단어는 제외하고 있는 단어만을 가지고 벡터로 만들 것이다."}],"\n",["$","p",null,{"children":"그럼 이 수치로 표현된 값을 어떻게 활용할까?"}],"\n",["$","p",null,{"children":"머신러닝 모델에 입력값으로 사용할 수도 있디만 단순히 계산만으로 문장간의 유사도(Sentence similarity)를 알 수도 있다."}],"\n",["$","p",null,{"children":[["$","code",null,{"children":"the game is fun"}]," 과 ",["$","code",null,{"children":"The game is interesting"}]," 문장의 유사도를 구해보자."]}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"문장"}],["$","th",null,{"children":"벡터값"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"the game is fun"}],["$","td",null,{"children":["[",["$","code",null,{"children":"1"}],", ",["$","code",null,{"children":"1"}],", ",["$","code",null,{"children":"1"}],", 1, 0, 0, 0]"]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"The game is interesting"}],["$","td",null,{"children":["[",["$","code",null,{"children":"1"}],", ",["$","code",null,{"children":"1"}],", ",["$","code",null,{"children":"1"}],", 0, 1, 0, 0]"]}]]}]]}]]}],"\n",["$","div",null,{"className":"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary","children":[false,["$","div",null,{"className":"callout-contents flex-1","children":["$undefined",["\n",["$","p",null,{"children":["$","strong",null,{"children":"유사도 = (1x1) + (1x1) + (1x1) + (1x0) + (0x1) + (0x0) + (0x0) = 3"}]}],"\n"]]}]]}],"\n",["$","p",null,{"children":["또한 수치로 표한된 값을 이용해 기계학습 모델에 입력값으로 사용할 수가 있다. 문장에 대한 감성분석을 해주는 모델이 있다면, 벡터화된 값을 모델에 입력값으로 사용할 수 있고 모델은 우리가 원하는 ",["$","code",null,{"children":"Good"}]," 또는 ",["$","code",null,{"children":"Bad"}],"의 결과를 출력해 줄 수 있다."]}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/nlp-basic/bow-ml.png","alt":"bow","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"bow"}]]}],"\n",["$","p",null,{"children":"하지만 BOW는 몇 가지 단점이 있다."}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","strong",null,{"children":"Sparsity"}]}],"\n"]}],"\n",["$","p",null,{"children":["실제 사전에는 100만개가 넘는 단어들이 있을 수도 있다. 그렇게 되면 벡터의 차원이 100만개가 넘어가기 때문에 실제 문장하나를 표현할 때 대부분의 값이 0이고 그외의 값들은 상당히 적을 것이다. 결국 학습량이 많아지고 컴퓨터 자원도 상당히 많이 사용하게 된다.",["$","br",null,{}],"\n","(the game is fun [1,1,1,1,0,0,0,0,0,0,0,,,,,,0,0,0,0,0])"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","strong",null,{"children":"빈번한 단어는 더 많은 힘을 가진다."}]}],"\n"]}],"\n",["$","p",null,{"children":"많이 출현한 단어는 힘이 세진다. 만약 의미없는 단어들이 많이 사용 되었다면 우리가 원하는 결과를 얻기는 어려울 것이다."}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","strong",null,{"children":"Out of vocabulary"}]}],"\n"]}],"\n",["$","p",null,{"children":"오타, 줄임말 등의 단어들이 포함되면 굉장히 난감해진다.^^;"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","strong",null,{"children":"단어의 순서가 무시됨"}]}],"\n"]}],"\n",["$","p",null,{"children":"단어의 출현 횟수만 셀수 있고 단어의 순서는 완전히 무시 된다. 단어의 순서가 무시된다는 것은 다른 의미를 가진 문장이 동일한 결과로 해석될 수 있다는 것이다."}],"\n",["$","p",null,{"children":"전혀 반대의 의미를 가진 두 문장을 보자."}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/nlp-basic/bow-02.png","alt":"nlp-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"nlp-1"}]]}],"\n",["$","p",null,{"children":["두 문장은 의미가 전혀 반대이지만 BOW를 이용해 처리한다면, 동일한 결과를 반환하게 될 것이다.",["$","br",null,{}],"\n","이런 단점을 보완하기 위해 좀더 개선된 n-gram이란 것이 있다. BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용하여 어느정도 단어의 순서를 반영 결과에 반영해 준다."]}],"\n",["$","h1",null,{"id":"n-gram","children":"N-Gram"}],"\n",["$","p",null,{"children":["BOW를 조금 더 개선하여 단어 하나만을 보는 것이 아니라 주변의 n개 단어를 뭉쳐서 보는 것이다. 뭉쳐진 n개의 단어들을 gram이라고 한다.",["$","br",null,{}],"\n","단어 개수에 따라 부르는 명칭이 다른데 2개의 단어를 묶어서 사용하면 ",["$","code",null,{"children":"bi-gram"}],", 3개면 ",["$","code",null,{"children":"tri-gram"}],"이라고 부른다.",["$","br",null,{}],"\n","(1-gram은 uni-gram이라고 한다.)"]}],"\n",["$","p",null,{"children":"다음 문장을 bi-gram를 사용하여 처리 한다면,"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"\"home run\""}]," 과 ",["$","strong",null,{"children":"\"run home\""}]]}],"\n",["$","div",null,{"className":"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary","children":[false,["$","div",null,{"className":"callout-contents flex-1","children":["$undefined",["\n",["$","p",null,{"children":["bag of words : [home, run] , [run, home]",["$","br",null,{}],"\n","bi-gram : [home run], [run home]"]}],"\n"]]}]]}],"\n",["$","p",null,{"children":"BOW를 사용한다면 두 문장은 같은 백터의 값을 갖게 되겠지만 bi-gram을 사용하면 2개를 뭉쳐서 사용하므로 어느정도의 순서가 보장되는 효과를 볼수 있게 되어 다른 결과 값을 가지게 될 것이다."}],"\n",["$","p",null,{"children":"이런 특성을 이용해 n-gram은 다음 단어 예측하거나 어떤 단어를 입력 했을때 오타를 발견하고 다른 단어를 추천해 주는데 활용할 수 있다."}],"\n",["$","h1",null,{"id":"텍스트-전처리-preprocessing","children":"텍스트 전처리 (Preprocessing)"}],"\n",["$","p",null,{"children":[["$","img",null,{"src":"/posts/ai/nlp-basic/process.png","alt":"nlp-1","className":"mx-auto mb-0 mt-8 rounded-md"}],["$","span",null,{"className":"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400","children":"nlp-1"}]]}],"\n",["$","p",null,{"children":"BOW나 n-gram이나 모두 많이 쓰이지만 가장 중요한 것은 단어의 전처리가 확실해야 한다는 것이다. 이 글에서는 설명을 위해 간단한 문장만을 사용하여 크게 신경을 쓸 필요는 없겠지만, 자연어 처리를 하다보면 다양한 케이스의 문장들을 접하게 될 것이며 이런 문장들을 토큰화하고 불필요한 단어들은 제거하고 같은 의미의 단어들은 치환하는 등의 고단한 작업 들을 해야 할 것이다. 하지만 다행인 것은 이런 전처리 작업들을 편하게 할 수 있도록 도와주는 좋은 라이브러리들이 있다."}],"\n",["$","p",null,{"children":"다음 포스팅에서는 전처리에 대해 자세히 살펴보도록 하겠다..."}]]
