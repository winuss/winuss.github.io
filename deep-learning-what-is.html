<!DOCTYPE html><html lang="ko" class="h-full scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/neuron.png"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/deep_01.png"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/deep_layer.png"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/image_net.png"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/deep_02.png"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/deep_03.png"/><link rel="preload" as="image" href="/posts/ai/deep-learning-what-is/deep_04.png"/><link rel="stylesheet" href="/_next/static/css/2e2884a4d67cc4fc.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-c579393c27fbbcf1.js" crossorigin=""/><script src="/_next/static/chunks/b40443eb-6913cff014374d9b.js" async="" crossorigin=""></script><script src="/_next/static/chunks/848-d93e525e2af78b30.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-a4a2ebec86a29373.js" async="" crossorigin=""></script><script src="/_next/static/chunks/605-9393b79bc12886be.js" async=""></script><script src="/_next/static/chunks/614-02ab0d044823ada6.js" async=""></script><script src="/_next/static/chunks/229-1ed242868e4072e6.js" async=""></script><script src="/_next/static/chunks/323-e456fd16a521999d.js" async=""></script><script src="/_next/static/chunks/641-e9f4e83f8ec0c2db.js" async=""></script><script src="/_next/static/chunks/app/layout-fbff880e07d7ab25.js" async=""></script><script src="/_next/static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-4GBTCVBGMX" as="script"/><link rel="preload" href="https://www.googletagmanager.com/gtm.js?id=GTM-M5JW97ZQ" as="script"/><title>딥러닝 - 딥러닝의 이해 | DevTimes</title><meta name="description" content="딥러닝의 개념에 대해 이해해보자."/><meta property="og:title" content="딥러닝 - 딥러닝의 이해 | DevTimes"/><meta property="og:description" content="딥러닝의 개념에 대해 이해해보자."/><meta property="og:url" content="https://devtimes.com/deep-learning-what-is"/><meta property="og:image" content="https://devtimes.com/posts/ai/deep-learning-what-is/deep_main.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2019-05-16T00:00:00.000Z"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="딥러닝 - 딥러닝의 이해 | DevTimes"/><meta name="twitter:description" content="딥러닝의 개념에 대해 이해해보자."/><meta name="twitter:image" content="https://devtimes.com/posts/ai/deep-learning-what-is/deep_main.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="518x518"/><link rel="icon" href="/icon.png?8d4a794a4e2a4e0b" type="image/png" sizes="518x518"/><link rel="apple-touch-icon" href="/apple-icon.png?8d4a794a4e2a4e0b" type="image/png" sizes="518x518"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="flex min-h-screen flex-col font-pretendard"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><nav style="margin-top:0" class="fixed z-40 flex w-full flex-col items-center justify-center border-b bg-background shadow-sm print:hidden"><div class="mt-1 flex h-[64px] w-full max-w-[1200px] items-center justify-between px-4"><div class="flex items-center font-medium"><a class="rounded-full px-4 py-1 text-center text-sm transition-colors hover:text-primary bg-muted font-medium text-primary" href="/">DEVTIMES</a><a class="rounded-full px-4 py-1 text-center text-sm transition-colors hover:text-primary text-muted-foreground" href="/about">About</a></div><div class="flex gap-3"><a target="_blank" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground aspect-square p-2" href="https://github.com/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github size-[1.2rem]"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div></div></nav><main class="mt-[calc(64px+env(safe-area-inset-top))] flex flex-1 flex-col"><div class="prose mx-auto w-full max-w-[750px] px-5 dark:prose-invert sm:px-6"><header class="mt-14 text-center"><h1 class="mb-5 text-3xl">딥러닝 - 딥러닝의 이해</h1><div class="mb-3 text-base"><a class="font-semibold text-pink-600 no-underline underline-offset-4 hover:underline" href="/ai">Ai</a></div><div class="flex justify-center gap-3 text-sm text-gray-500 dark:text-gray-400"><div class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar-days w-3.5"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path><path d="M8 14h.01"></path><path d="M12 14h.01"></path><path d="M16 14h.01"></path><path d="M8 18h.01"></path><path d="M12 18h.01"></path><path d="M16 18h.01"></path></svg><span>2019년 05월 16일</span></div><div class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock3 w-3.5"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16.5 12"></polyline></svg><span>13<!-- -->분</span></div></div><hr class="mt-5"/></header><article class="relative"><aside class="not-prose absolute -top-[200px] left-full -mb-[100px] hidden h-[calc(100%+150px)] xl:block "><div class="sticky bottom-0  top-[200px] z-10 ml-[5rem] mt-[200px] w-[200px]"><div class="mb-4 border-l px-4 py-2"><div class="mb-1 font-bold">On this page</div><ul class="text-xs"></ul></div><div class="flex gap-2"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-to-line "><path d="M5 3h14"></path><path d="m18 13-6-6-6 6"></path><path d="M12 7v14"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square-text "><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path><path d="M13 8H7"></path><path d="M17 12H7"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2"><span class="sr-only">Copy</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy "><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div></aside><h1 id="개념">개념</h1>
<p><code>심층 신경망을 이용한 머신러닝 기법</code></p>
<p>딥러닝이라는 용어는 2006년 캐나다 토론토 대학교의 제프리 힌튼(Geoffrey Hinton) 교수의 논문을 통해 처음 사용이 되었다. 하지만 딥러닝은 사실 새로운 개념이 아니다. 오래전부터 있어오던 인공신경망(Artificial Neural Network, ANN)과 크게 다를 바 없다. &#x27;인공신경망&#x27;이라고 하면 복잡한 뇌 구조를 생각하면서 어렵게 생각이 들겠지만 실제 뉴런의 행동을 살펴보면 어떻게 이렇게 단순한 행동으로 이루어질까 싶을 정도로 심플하다.</p>
<p><img src="/posts/ai/deep-learning-what-is/neuron.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span></p>
<p>입력신호가 오면 길이에 따른 가중치(weight)를 부여하고 임계값(bias)을 더한 후 축색돌기를 통해 이동한다. 그리고 그 값이 조건에 만족한다면 다음 뉴런으로 넘어가고 그렇지 않다면 넘어가지 않게 된다. 이러한 실제 뉴런에서 이뤄지는 과정을 그대로 수학적으로 구현한 것이다.</p>
<p>컴퓨터가 사진 속에서 고양이를 검출해내야 한다고 생각해보자. &#x27;고양이&#x27;라는 추상적 이미지는 아마 선, 면, 형상, 색깔, 크기 등 다양한 요소들이 조합된 결과물일 것이다. 이것은 아마 &#x27;선 30cm 이상은 고양이, 이하는 고양이 아님&#x27;, 또는 &#x27;갈색은 고양이, 빨간색은 고양이 아님&#x27; 처럼 간단한 선형 구분으로는 식별해 낼 수 없는 문제이다. 딥러닝은 이 과제를 선 긋고 왜곡하고 합하고를 반복하며 복잡한 공간 속에서의 최적의 구분선을 만들어 내는 목적을 가지고 있다.</p>
<p><img src="/posts/ai/deep-learning-what-is/deep_01.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span></p>
<p>파란선과 빨간선의 영역을 구분한다고 생각해보자. 그냥 구분 선을 긋는다면 아마 왼쪽처럼 불완전하게 그을 수 있을 것이다. 하지만 공간을 왜곡하면 오른쪽 같이 아름답게 구분선을 그릴 수 있다. 이처럼 인공신경망은 선 긋고, 구기고, 합하고를 반복하여 데이터를 처리한다.</p>
<p>그럼 어떠한 규칙으로 선을 긋고 공간을 왜곡할까? 바로 데이터에 근거하는 것이다. 일단 대충 선을 긋고 그것들을 살살 움직여가며 구분 결과가 더 좋게 나오도록 선을 움직이는 것인데 이러한 과정을 최적화(optimization)이라고 한다. 딥러닝은 아주 많은 데이터와 아주 오랜 시간의 최적화를 통해 데이터를 학습하게 된다. (양에는 장사 없다고나 할까...)</p>
<p><img src="/posts/ai/deep-learning-what-is/deep_layer.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span><br/>
<!-- -->딥러닝은 하나의 뉴런이 아닌 수천 수억개의 뉴런들이 연결되어 이뤄진다. 즉 인공 신경 또한 수많은 연결들을 통해 망을 이룬다. 크게 입력층(input layer)과 출력층(output layer) 그리고 그 사이에 있는 layer들을 묶어서 은닉층(hidden layer)이라고 한다. 위 그림에서는 모든 노드가 연결되어있지만, 꼭 다 연결되어있어야 하는 것은 아니다. 단지 내가 어떻게 신경망을 구성하냐에 따라 정해지는 것이다.</p>
<p>저렇게 망이 이뤄져 있는 것은 알겠는데, 대체 어떻게 학습을 한다는건지 이해가 안 될 수 있다.<br/>
<!-- -->내가 고양이 사진을 입력에 넣었다면 출력에는 고양이라는 결과가 나와야 맞는 것이다. 고양이가 맞다면 그냥 넘어가면 되지만, 고양이가 아니라고 결과가 나왔다면 이것은 문제가 있는 것이다. 따라서 다시 뒤로 돌아가면서 각 인공 신경의 가중치(weight)와 임계값(bias) 값을 수정하게 된다. 이러한 과정을 반복하며 weight와 bias 값을 조정하고 고양이라는 결과가 나오도록 &#x27;학습&#x27; 하는 것이다.</p>
<p><img src="/posts/ai/deep-learning-what-is/image_net.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span></p>
<p>2012년 이미지넷에서 이미지 분류 대회가 있었다. 1000개의 카테고리와 100만개의 이미지로 구성되어 분류 정확도를 겨루는 대회이고, 아래 사진만 보더라도 딥러닝이 이미지 인식 부문에서 얼마나 큰 영향을 끼치고 있는지 알 수 있다. 이 대회 이전에는 기계의 이미지 인식률이 75%를 넘지 못했고 80% 이상의 인식률은 불가능이라는 인식이 있었다. 하지만 이 대회에서 힌튼 교수의 제자 알렉스가 알렉스넷(AlexNet)이라는 딥러닝 기반 알고리즘으로 84.7%를 찍었고, 이후로는 대부분의 참가자들이 딥러닝으로 방향을 돌렸다. 현재는 오류율이 5% 이하의 정확도로 인간의 인식수준을 뛰어넘었다.</p>
<p>딥러닝이 대단한 것은 일반적인 기계 학습과 달리 특징 추출(feature extraction)이 자동적으로 이루어지는 점 이다. 기존에는 효과적인 특징을 추출하기 위해 관련 분야 전문가가 오랜 시간동안 직접 특징을 추출하는 수식이나 방법을 고안해야 했다. 이 방법은 개발, 평가 및 보완에 많은 시간이 걸리는데 딥러닝은 이런 과정을 컴퓨터가 대신 하도록 알고리즘을 짠 것으로, 사람에 비해 훨씬 빠르고 효과적으로 수행해도록 학습시켜준다.</p>
<h1 id="발전과정">발전과정</h1>
<p>딥러닝의 발전과정을 보면 아주 오래전부터 시작이 되었고 지금에 이르기까지 많은 이들의 노력이 있었다.</p>
<p><code>1957 : 최초 신경망 모델 Perceptron 등장</code></p>
<p>퍼셉트론은 인간의 두뇌 움직임을 수학적으로 구성하였다.</p>
<p><img src="/posts/ai/deep-learning-what-is/deep_02.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span></p>
<p><code>1957 ~ 1986 : 첫 빙하기(30년)</code></p>
<p><img src="/posts/ai/deep-learning-what-is/deep_03.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span></p>
<p>그 당시 AND와 OR의 경우 퍼셉트론을 통한 선형 분리(linearly separable)가 가능했다. 따라서 AND / OR 연산이 가능하게 훈련한 퍼셉트론을 보고 많은 사람들은 기계의 학습 가능성에 대해 큰 기대를 하게 되었다.<br/>
<!-- -->하지만 이러한 그들의 엄청난 기대에 찬물을 끼얹어버린 것이 XOR 연산에 대한 불가능이었다. 당시 하나의 인공 신경. 즉 퍼셉트론으로는 선형 분리가 불가능해 XOR 연산에 대한 학습이 불가능했다. </p>
<p>1969년 이 문제를 해결하기 위해 퍼셉트론을 다중으로 겹치면 이 문제를 해결할 수 있음을 증명한 <strong>다중 계층 퍼셉트론이 등장</strong>하지만 레이어가 복잡해질수록 연산이 복잡해져서 현실적으로 파라미터값을 구하는 것이 불가능 하였다.</p>
<p><code>1986 : 새로운 학습 방법 등장</code></p>
<p>데이터가 모델을 스스로 찾아내는 역전파(Backpropagation) 등장<br/>
<!-- -->즉 앞의 진행방향에서 고쳐가는 것이 아니라 결과를 보고 뒤로 가면서 weight와 bias를 조정하는 방법을 고안 하였고, XOR 뿐 아니라 좀 더 복잡한 과정도 해결할 수 있음을 보이며 다시 인공 신경망은 사람들의 관심을 끌기 시작했다.</p>
<p><code>1990 : BOOM</code></p>
<p><img src="/posts/ai/deep-learning-what-is/deep_04.png" alt="deep-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">deep-1</span><br/>
<!-- -->(터미네이터2 심판의 날 (1991) 대사 中)</p>
<p>영화 중반부에 터미네이터인 아놀드의 대사 중에 &quot;My CPU is a neural-net processor.&quot; 라는 말이 나온다. 스스로 학습할 수 있는 기계를 neural-net processor 라는 말로 설명했고 이 당시에도 neural network에 관심이 있었음을 알 수 있다. </p>
<p><code>1990 ~ 2000 : 두번째 빙하기(10년)</code><br/>
<!-- -->신경망이 깊어질수록 원하는 결과를 얻을 수 없다. 오히려 성능이 저하되는 경우가 발생.</p>
<p>다음의 문제를 해결하는데 10년이 걸리게 된다.</p>
<ol>
<li>Overfitting (과하거나)</li>
<li>Vanishing Gradient (덜하거나)</li>
<li>Too slow  (느리거나)</li>
</ol>
<p>이런 문제를 해결하기 위해 연구하는 10년 동안 GUP를 활용한 연산시도, 알고리즘의 발전이 있게 된다.(SVM, Random Forest 등장)</p>
<p><code>2000 : 3가지 한계점의 해결방안 등장</code></p>
<ol>
<li>과한 적합 해결<!-- -->
<ul>
<li>Droupout</li>
</ul>
</li>
<li>덜한 적합 해결<!-- -->
<ul>
<li>ReLU(Rectified Linear Unit) : 수렴속도가 시그모이드류 함수 대비 6배 빠름</li>
</ul>
</li>
<li>느린 적합 해결<!-- -->
<ul>
<li>Adam : 확룰적 경사 하강법(SGD)에서 더 나아가 학습속도와 운동량을 고려한 옵티마이저가 등장</li>
</ul>
</li>
</ol>
<p><code>2006 : 드디어 Deep Network. Deep Learning 용어가 사용되기 시작</code></p>
<p>이러한 여러번의 실패로 인해 이후 나온 논문에서는 neural net이라는 단어를 찾을 수 없었고 neural 대신 deep 이라는 단어를 사용했다. 이 당시 neural network 라는 말만 들어가면 논문에서 거절당한다는 말이 있을 정도로 신경망이 외면받던 시기였기 때문에 좀 더 사람들의 이목을 끌 수 있는 단어를 택했던 것 같다.</p>
<h1 id="마치며">마치며</h1>
<p>딥 러닝이 화두가 되고 있는 것은 비교적 최근의 일이지만, 딥 러닝의 기본 구조인 인공 신경망(Artificial Neural Network)의 역사는 오래되었다. 1957년에 등장한 초기 신경망인 퍼셉트론(Perceptron)으로 시작해 최근에 쓰이고 있는 발전된 신경망 RNN, LSTM에 대해서도 좀더 자세히 알아보면 좋을 것 같다.</p></article><hr/><section></section><div class="group fixed bottom-4 right-4 xl:hidden"><div class="group relative flex flex-col-reverse"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 z-10 transition"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-bolt "><path d="M21 16V8a2 2 0 0 0-1-1.73l-7-4a2 2 0 0 0-2 0l-7 4A2 2 0 0 0 3 8v8a2 2 0 0 0 1 1.73l7 4a2 2 0 0 0 2 0l7-4A2 2 0 0 0 21 16z"></path><circle cx="12" cy="12" r="4"></circle></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 transition"><span class="sr-only">Copy</span><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy "><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 transition"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square-text "><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path><path d="M13 8H7"></path><path d="M17 12H7"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 transition"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-to-line "><path d="M5 3h14"></path><path d="m18 13-6-6-6 6"></path><path d="M12 7v14"></path></svg></button></div></div></div></main><footer class="mb-16 mt-20 flex flex-col items-center justify-center gap-4 text-center print:hidden"><div class="flex justify-center gap-4"><a target="_blank" href="https://github.com/"><svg fill="currentColor" viewBox="0 0 16 16" height="30" width="30" class="fill-foreground transition hover:fill-pink-600"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a target="_blank" href="https://www.linkedin.com/in/"><svg viewBox="0 0 24 24" fill="currentColor" height="30" width="30" class="fill-foreground transition hover:fill-pink-600"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div><div>© 2024. <span class="font-semibold">Devtimes</span> all rights reserved.</div></footer><div role="region" aria-label="Notifications (F8)" tabindex="-1" style="pointer-events:none"><ol tabindex="-1" class="fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[300px]"></ol></div><script src="/_next/static/chunks/webpack-c579393c27fbbcf1.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/2e2884a4d67cc4fc.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:I[2386,[],\"\"]\n6:I[9261,[],\"\"]\n8:I[6675,[],\"\"]\n9:I[8390,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"229\",\"static/chunks/229-1ed242868e4072e6.js\",\"323\",\"static/chunks/323-e456fd16a521999d.js\",\"641\",\"static/chunks/641-e9f4e83f8ec0c2db.js\",\"185\",\"static/chunks/app/layout-fbff880e07d7ab25.js\"],\"ThemeProvider\"]\na:I[7083,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"229\",\"static/chunks/229-1ed242868e4072e6.js\",\"323\""])</script><script>self.__next_f.push([1,",\"static/chunks/323-e456fd16a521999d.js\",\"641\",\"static/chunks/641-e9f4e83f8ec0c2db.js\",\"185\",\"static/chunks/app/layout-fbff880e07d7ab25.js\"],\"Header\"]\nb:I[9605,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js\"],\"\"]\nc:I[3760,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"229\",\"static/chunks/229-1ed242868e4072e6.js\",\"323\",\"static/chunks/323-e456fd16a521"])</script><script>self.__next_f.push([1,"999d.js\",\"641\",\"static/chunks/641-e9f4e83f8ec0c2db.js\",\"185\",\"static/chunks/app/layout-fbff880e07d7ab25.js\"],\"Toaster\"]\nd:I[450,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"229\",\"static/chunks/229-1ed242868e4072e6.js\",\"323\",\"static/chunks/323-e456fd16a521999d.js\",\"641\",\"static/chunks/641-e9f4e83f8ec0c2db.js\",\"185\",\"static/chunks/app/layout-fbff880e07d7ab25.js\"],\"GoogleAnalytics\"]\ne:I[8806,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614"])</script><script>self.__next_f.push([1,"-02ab0d044823ada6.js\",\"229\",\"static/chunks/229-1ed242868e4072e6.js\",\"323\",\"static/chunks/323-e456fd16a521999d.js\",\"641\",\"static/chunks/641-e9f4e83f8ec0c2db.js\",\"185\",\"static/chunks/app/layout-fbff880e07d7ab25.js\"],\"GoogleTagManager\"]\n10:I[5404,[],\"\"]\n7:[\"slug\",\"deep-learning-what-is\",\"d\"]\n11:[]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/2e2884a4d67cc4fc.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"tHvGB-DuNJX99nTIek84U\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/deep-learning-what-is\",\"initialTree\":[\"\",{\"children\":[\"(blog)\",{\"children\":[[\"slug\",\"deep-learning-what-is\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"deep-learning-what-is\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"(blog)\",{\"children\":[[\"slug\",\"deep-learning-what-is\",\"d\"],{\"children\":[\"__PAGE__\",{},[\"$L4\",\"$L5\",null]]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blog)\",\"children\",\"$7\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blog)\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"h-full scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"flex min-h-screen flex-col font-pretendard\",\"children\":[[\"$\",\"$L9\",null,{\"children\":[[\"$\",\"$La\",null,{}],[\"$\",\"main\",null,{\"className\":\"mt-[calc(64px+env(safe-area-inset-top))] flex flex-1 flex-col\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"grid flex-1 place-content-center text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-4 text-2xl font-bold\",\"children\":\"Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-8 text-lg\",\"children\":\"찾을 수 없는 페이지입니다.\"}],[\"$\",\"$Lb\",null,{\"href\":\"/\",\"children\":\"홈으로\",\"className\":\"inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 bg-primary text-primary-foreground hover:bg-primary/90 h-10 py-2 mx-auto w-fit px-10\"}]]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"footer\",null,{\"className\":\"mb-16 mt-20 flex flex-col items-center justify-center gap-4 text-center print:hidden\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-4\",\"children\":[[\"$\",\"$Lb\",null,{\"href\":\"https://github.com/\",\"target\":\"_blank\",\"children\":[\"$\",\"svg\",null,{\"fill\":\"currentColor\",\"viewBox\":\"0 0 16 16\",\"height\":30,\"width\":30,\"className\":\"fill-foreground transition hover:fill-pink-600\",\"children\":[\"$\",\"path\",null,{\"d\":\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0016 8c0-4.42-3.58-8-8-8z\"}]}]}],[\"$\",\"$Lb\",null,{\"href\":\"https://www.linkedin.com/in/\",\"target\":\"_blank\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"fill\":\"currentColor\",\"height\":30,\"width\":30,\"className\":\"fill-foreground transition hover:fill-pink-600\",\"children\":[\"$\",\"path\",null,{\"d\":\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"}]}]}]]}],[\"$\",\"div\",null,{\"children\":[\"© 2024. \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Devtimes\"}],\" all rights reserved.\"]}]]}]]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{\"gaId\":\"G-4GBTCVBGMX\"}],[\"$\",\"$Le\",null,{\"gtmId\":\"GTM-M5JW97ZQ\"}]]}]}],null]],\"initialHead\":[false,\"$Lf\"],\"globalErrorComponent\":\"$10\",\"missingSlots\":\"$W11\"}]]\n"])</script><script>self.__next_f.push([1,"12:I[2465,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js\"],\"\"]\n14:I[3574,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-d29344baf3c6eb9f.js\"],\"\"]\n15:I[938,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"614\",\"static/chunks/614-02ab0d044823ada6.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-d29344ba"])</script><script>self.__next_f.push([1,"f3c6eb9f.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"prose mx-auto w-full max-w-[750px] px-5 dark:prose-invert sm:px-6\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mt-14 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-5 text-3xl\",\"children\":\"딥러닝 - 딥러닝의 이해\"}],[\"$\",\"div\",null,{\"className\":\"mb-3 text-base\",\"children\":[\"$\",\"$Lb\",null,{\"href\":\"/ai\",\"className\":\"font-semibold text-pink-600 no-underline underline-offset-4 hover:underline\",\"children\":\"Ai\"}]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-3 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-calendar-days w-3.5\",\"children\":[[\"$\",\"path\",\"1cmpym\",{\"d\":\"M8 2v4\"}],[\"$\",\"path\",\"4m81vk\",{\"d\":\"M16 2v4\"}],[\"$\",\"rect\",\"1hopcy\",{\"width\":\"18\",\"height\":\"18\",\"x\":\"3\",\"y\":\"4\",\"rx\":\"2\"}],[\"$\",\"path\",\"8toen8\",{\"d\":\"M3 10h18\"}],[\"$\",\"path\",\"6423bh\",{\"d\":\"M8 14h.01\"}],[\"$\",\"path\",\"1etili\",{\"d\":\"M12 14h.01\"}],[\"$\",\"path\",\"1gbofw\",{\"d\":\"M16 14h.01\"}],[\"$\",\"path\",\"lrp35t\",{\"d\":\"M8 18h.01\"}],[\"$\",\"path\",\"mhygvu\",{\"d\":\"M12 18h.01\"}],[\"$\",\"path\",\"kzsmim\",{\"d\":\"M16 18h.01\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"2019년 05월 16일\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-clock3 w-3.5\",\"children\":[[\"$\",\"circle\",\"1mglay\",{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"polyline\",\"1aq6pp\",{\"points\":\"12 6 12 12 16.5 12\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":[13,\"분\"]}]]}]]}],[\"$\",\"hr\",null,{\"className\":\"mt-5\"}]]}],null,[\"$\",\"article\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"$L12\",null,{\"toc\":[]}],\"$L13\"]}],[\"$\",\"hr\",null,{}],[\"$\",\"$L14\",null,{}],[\"$\",\"$L15\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1, viewport-fit=cover\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"딥러닝 - 딥러닝의 이해 | DevTimes\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"딥러닝의 개념에 대해 이해해보자.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"딥러닝 - 딥러닝의 이해 | DevTimes\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"딥러닝의 개념에 대해 이해해보자.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://devtimes.com/deep-learning-what-is\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://devtimes.com/posts/ai/deep-learning-what-is/deep_main.png\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"9\",{\"property\":\"article:published_time\",\"content\":\"2019-05-16T00:00:00.000Z\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"딥러닝 - 딥러닝의 이해 | DevTimes\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"딥러닝의 개념에 대해 이해해보자.\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:image\",\"content\":\"https://devtimes.com/posts/ai/deep-learning-what-is/deep_main.png\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"518x518\"}],[\"$\",\"link\",\"15\",{\"rel\":\"icon\",\"href\":\"/icon.png?8d4a794a4e2a4e0b\",\"type\":\"image/png\",\"sizes\":\"518x518\"}],[\"$\",\"link\",\"16\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?8d4a794a4e2a4e0b\",\"type\":\"image/png\",\"sizes\":\"518x518\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"h1\",null,{\"id\":\"개념\",\"children\":\"개념\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"심층 신경망을 이용한 머신러닝 기법\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"딥러닝이라는 용어는 2006년 캐나다 토론토 대학교의 제프리 힌튼(Geoffrey Hinton) 교수의 논문을 통해 처음 사용이 되었다. 하지만 딥러닝은 사실 새로운 개념이 아니다. 오래전부터 있어오던 인공신경망(Artificial Neural Network, ANN)과 크게 다를 바 없다. '인공신경망'이라고 하면 복잡한 뇌 구조를 생각하면서 어렵게 생각이 들겠지만 실제 뉴런의 행동을 살펴보면 어떻게 이렇게 단순한 행동으로 이루어질까 싶을 정도로 심플하다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/neuron.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"입력신호가 오면 길이에 따른 가중치(weight)를 부여하고 임계값(bias)을 더한 후 축색돌기를 통해 이동한다. 그리고 그 값이 조건에 만족한다면 다음 뉴런으로 넘어가고 그렇지 않다면 넘어가지 않게 된다. 이러한 실제 뉴런에서 이뤄지는 과정을 그대로 수학적으로 구현한 것이다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"컴퓨터가 사진 속에서 고양이를 검출해내야 한다고 생각해보자. '고양이'라는 추상적 이미지는 아마 선, 면, 형상, 색깔, 크기 등 다양한 요소들이 조합된 결과물일 것이다. 이것은 아마 '선 30cm 이상은 고양이, 이하는 고양이 아님', 또는 '갈색은 고양이, 빨간색은 고양이 아님' 처럼 간단한 선형 구분으로는 식별해 낼 수 없는 문제이다. 딥러닝은 이 과제를 선 긋고 왜곡하고 합하고를 반복하며 복잡한 공간 속에서의 최적의 구분선을 만들어 내는 목적을 가지고 있다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/deep_01.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"파란선과 빨간선의 영역을 구분한다고 생각해보자. 그냥 구분 선을 긋는다면 아마 왼쪽처럼 불완전하게 그을 수 있을 것이다. 하지만 공간을 왜곡하면 오른쪽 같이 아름답게 구분선을 그릴 수 있다. 이처럼 인공신경망은 선 긋고, 구기고, 합하고를 반복하여 데이터를 처리한다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"그럼 어떠한 규칙으로 선을 긋고 공간을 왜곡할까? 바로 데이터에 근거하는 것이다. 일단 대충 선을 긋고 그것들을 살살 움직여가며 구분 결과가 더 좋게 나오도록 선을 움직이는 것인데 이러한 과정을 최적화(optimization)이라고 한다. 딥러닝은 아주 많은 데이터와 아주 오랜 시간의 최적화를 통해 데이터를 학습하게 된다. (양에는 장사 없다고나 할까...)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/deep_layer.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]],[\"$\",\"br\",null,{}],\"\\n\",\"딥러닝은 하나의 뉴런이 아닌 수천 수억개의 뉴런들이 연결되어 이뤄진다. 즉 인공 신경 또한 수많은 연결들을 통해 망을 이룬다. 크게 입력층(input layer)과 출력층(output layer) 그리고 그 사이에 있는 layer들을 묶어서 은닉층(hidden layer)이라고 한다. 위 그림에서는 모든 노드가 연결되어있지만, 꼭 다 연결되어있어야 하는 것은 아니다. 단지 내가 어떻게 신경망을 구성하냐에 따라 정해지는 것이다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"저렇게 망이 이뤄져 있는 것은 알겠는데, 대체 어떻게 학습을 한다는건지 이해가 안 될 수 있다.\",[\"$\",\"br\",null,{}],\"\\n\",\"내가 고양이 사진을 입력에 넣었다면 출력에는 고양이라는 결과가 나와야 맞는 것이다. 고양이가 맞다면 그냥 넘어가면 되지만, 고양이가 아니라고 결과가 나왔다면 이것은 문제가 있는 것이다. 따라서 다시 뒤로 돌아가면서 각 인공 신경의 가중치(weight)와 임계값(bias) 값을 수정하게 된다. 이러한 과정을 반복하며 weight와 bias 값을 조정하고 고양이라는 결과가 나오도록 '학습' 하는 것이다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/image_net.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"2012년 이미지넷에서 이미지 분류 대회가 있었다. 1000개의 카테고리와 100만개의 이미지로 구성되어 분류 정확도를 겨루는 대회이고, 아래 사진만 보더라도 딥러닝이 이미지 인식 부문에서 얼마나 큰 영향을 끼치고 있는지 알 수 있다. 이 대회 이전에는 기계의 이미지 인식률이 75%를 넘지 못했고 80% 이상의 인식률은 불가능이라는 인식이 있었다. 하지만 이 대회에서 힌튼 교수의 제자 알렉스가 알렉스넷(AlexNet)이라는 딥러닝 기반 알고리즘으로 84.7%를 찍었고, 이후로는 대부분의 참가자들이 딥러닝으로 방향을 돌렸다. 현재는 오류율이 5% 이하의 정확도로 인간의 인식수준을 뛰어넘었다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"딥러닝이 대단한 것은 일반적인 기계 학습과 달리 특징 추출(feature extraction)이 자동적으로 이루어지는 점 이다. 기존에는 효과적인 특징을 추출하기 위해 관련 분야 전문가가 오랜 시간동안 직접 특징을 추출하는 수식이나 방법을 고안해야 했다. 이 방법은 개발, 평가 및 보완에 많은 시간이 걸리는데 딥러닝은 이런 과정을 컴퓨터가 대신 하도록 알고리즘을 짠 것으로, 사람에 비해 훨씬 빠르고 효과적으로 수행해도록 학습시켜준다.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"발전과정\",\"children\":\"발전과정\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"딥러닝의 발전과정을 보면 아주 오래전부터 시작이 되었고 지금에 이르기까지 많은 이들의 노력이 있었다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"1957 : 최초 신경망 모델 Perceptron 등장\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"퍼셉트론은 인간의 두뇌 움직임을 수학적으로 구성하였다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/deep_02.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"1957 ~ 1986 : 첫 빙하기(30년)\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/deep_03.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"그 당시 AND와 OR의 경우 퍼셉트론을 통한 선형 분리(linearly separable)가 가능했다. 따라서 AND / OR 연산이 가능하게 훈련한 퍼셉트론을 보고 많은 사람들은 기계의 학습 가능성에 대해 큰 기대를 하게 되었다.\",[\"$\",\"br\",null,{}],\"\\n\",\"하지만 이러한 그들의 엄청난 기대에 찬물을 끼얹어버린 것이 XOR 연산에 대한 불가능이었다. 당시 하나의 인공 신경. 즉 퍼셉트론으로는 선형 분리가 불가능해 XOR 연산에 대한 학습이 불가능했다. \"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"1969년 이 문제를 해결하기 위해 퍼셉트론을 다중으로 겹치면 이 문제를 해결할 수 있음을 증명한 \",[\"$\",\"strong\",null,{\"children\":\"다중 계층 퍼셉트론이 등장\"}],\"하지만 레이어가 복잡해질수록 연산이 복잡해져서 현실적으로 파라미터값을 구하는 것이 불가능 하였다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"1986 : 새로운 학습 방법 등장\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"데이터가 모델을 스스로 찾아내는 역전파(Backpropagation) 등장\",[\"$\",\"br\",null,{}],\"\\n\",\"즉 앞의 진행방향에서 고쳐가는 것이 아니라 결과를 보고 뒤로 가면서 weight와 bias를 조정하는 방법을 고안 하였고, XOR 뿐 아니라 좀 더 복잡한 과정도 해결할 수 있음을 보이며 다시 인공 신경망은 사람들의 관심을 끌기 시작했다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"1990 : BOOM\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[[\"$\",\"img\",null,{\"src\":\"/posts/ai/deep-learning-what-is/deep_04.png\",\"alt\":\"deep-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"deep-1\"}]],[\"$\",\"br\",null,{}],\"\\n\",\"(터미네이터2 심판의 날 (1991) 대사 中)\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"영화 중반부에 터미네이터인 아놀드의 대사 중에 \\\"My CPU is a neural-net processor.\\\" 라는 말이 나온다. 스스로 학습할 수 있는 기계를 neural-net processor 라는 말로 설명했고 이 당시에도 neural network에 관심이 있었음을 알 수 있다. \"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"1990 ~ 2000 : 두번째 빙하기(10년)\"}],[\"$\",\"br\",null,{}],\"\\n\",\"신경망이 깊어질수록 원하는 결과를 얻을 수 없다. 오히려 성능이 저하되는 경우가 발생.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"다음의 문제를 해결하는데 10년이 걸리게 된다.\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Overfitting (과하거나)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Vanishing Gradient (덜하거나)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Too slow  (느리거나)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이런 문제를 해결하기 위해 연구하는 10년 동안 GUP를 활용한 연산시도, 알고리즘의 발전이 있게 된다.(SVM, Random Forest 등장)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"2000 : 3가지 한계점의 해결방안 등장\"}]}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"과한 적합 해결\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Droupout\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"덜한 적합 해결\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"ReLU(Rectified Linear Unit) : 수렴속도가 시그모이드류 함수 대비 6배 빠름\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"느린 적합 해결\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Adam : 확룰적 경사 하강법(SGD)에서 더 나아가 학습속도와 운동량을 고려한 옵티마이저가 등장\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"2006 : 드디어 Deep Network. Deep Learning 용어가 사용되기 시작\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이러한 여러번의 실패로 인해 이후 나온 논문에서는 neural net이라는 단어를 찾을 수 없었고 neural 대신 deep 이라는 단어를 사용했다. 이 당시 neural network 라는 말만 들어가면 논문에서 거절당한다는 말이 있을 정도로 신경망이 외면받던 시기였기 때문에 좀 더 사람들의 이목을 끌 수 있는 단어를 택했던 것 같다.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"마치며\",\"children\":\"마치며\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"딥 러닝이 화두가 되고 있는 것은 비교적 최근의 일이지만, 딥 러닝의 기본 구조인 인공 신경망(Artificial Neural Network)의 역사는 오래되었다. 1957년에 등장한 초기 신경망인 퍼셉트론(Perceptron)으로 시작해 최근에 쓰이고 있는 발전된 신경망 RNN, LSTM에 대해서도 좀더 자세히 알아보면 좋을 것 같다.\"}]]\n"])</script><script>self.__next_f.push([1,""])</script></body></html>