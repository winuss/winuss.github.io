<!DOCTYPE html><html lang="ko" class="h-full scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"/><link rel="preload" as="image" href="/posts/ai/nlp-basic/bow-01.png"/><link rel="preload" as="image" href="/posts/ai/nlp-basic/bow-ml.png"/><link rel="preload" as="image" href="/posts/ai/nlp-basic/bow-02.png"/><link rel="preload" as="image" href="/posts/ai/nlp-basic/process.png"/><link rel="stylesheet" href="/_next/static/css/dfa5ebae0ef69c34.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-5ffb43b53f25eb9b.js" crossorigin=""/><script src="/_next/static/chunks/b40443eb-6913cff014374d9b.js" async="" crossorigin=""></script><script src="/_next/static/chunks/848-d93e525e2af78b30.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-a4a2ebec86a29373.js" async="" crossorigin=""></script><script src="/_next/static/chunks/605-9393b79bc12886be.js" async=""></script><script src="/_next/static/chunks/999-ff03068a6bad5acb.js" async=""></script><script src="/_next/static/chunks/332-a5eeb5385fe2fce8.js" async=""></script><script src="/_next/static/chunks/672-63761e12710ab043.js" async=""></script><script src="/_next/static/chunks/319-a036a883e9a56048.js" async=""></script><script src="/_next/static/chunks/app/layout-673adf006acb933b.js" async=""></script><script src="/_next/static/chunks/app/(blog)/%5Bslug%5D/page-0b2148024efdb29d.js" async=""></script><link rel="preload" href="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1154659137489563" as="script"/><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-4GBTCVBGMX" as="script"/><link rel="preload" href="https://www.googletagmanager.com/gtm.js?id=GTM-M5JW97ZQ" as="script"/><title>NLP - Bag of words, n-gram | DevTimes</title><meta name="description" content="NLP - Bag of words, n-gram"/><meta property="og:title" content="NLP - Bag of words, n-gram | DevTimes"/><meta property="og:description" content="NLP - Bag of words, n-gram"/><meta property="og:url" content="https://devtimes.com/nlp-basic"/><meta property="og:image" content="https://devtimes.com/posts/ai/nlp-basic/cover.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2019-08-07T00:00:00.000Z"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="NLP - Bag of words, n-gram | DevTimes"/><meta name="twitter:description" content="NLP - Bag of words, n-gram"/><meta name="twitter:image" content="https://devtimes.com/posts/ai/nlp-basic/cover.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="518x518"/><link rel="icon" href="/icon.png?8d4a794a4e2a4e0b" type="image/png" sizes="518x518"/><link rel="apple-touch-icon" href="/apple-icon.png?8d4a794a4e2a4e0b" type="image/png" sizes="518x518"/><script>(self.__next_s=self.__next_s||[]).push(["https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1154659137489563",{"crossOrigin":"anonymous","id":"adsbygoogle-init"}])</script><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="flex min-h-screen flex-col font-pretendard"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><nav style="margin-top:0" class="fixed z-40 flex w-full flex-col items-center justify-center border-b bg-background shadow-sm print:hidden"><div class="mt-1 flex h-[64px] w-full max-w-[1200px] items-center justify-between px-4"><div class="flex items-center font-medium"><a class="rounded-full px-4 py-1 text-center text-sm transition-colors hover:text-primary bg-muted font-medium text-primary" href="/">DEVTIMES</a><a class="rounded-full px-4 py-1 text-center text-sm transition-colors hover:text-primary text-muted-foreground" href="/about">About</a></div><div class="flex gap-3"><a target="_blank" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground aspect-square p-2" href="https://github.com/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github size-[1.2rem]"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div></div></nav><main class="mt-[calc(64px+env(safe-area-inset-top))] flex flex-1 flex-col"><div class="prose mx-auto w-full max-w-[750px] px-5 dark:prose-invert sm:px-6"><header class="mt-14 text-center"><h1 class="mb-5 text-3xl">NLP - Bag of words, n-gram</h1><div class="mb-3 text-base"><a class="font-semibold text-pink-600 no-underline underline-offset-4 hover:underline" href="/ai">Ai</a></div><div class="flex justify-center gap-3 text-sm text-gray-500 dark:text-gray-400"><div class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar-days w-3.5"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path><path d="M8 14h.01"></path><path d="M12 14h.01"></path><path d="M16 14h.01"></path><path d="M8 18h.01"></path><path d="M12 18h.01"></path><path d="M16 18h.01"></path></svg><span>2019년 08월 07일</span></div><div class="flex items-center gap-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock3 w-3.5"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16.5 12"></polyline></svg><span>9<!-- -->분</span></div></div><hr class="mt-5"/></header><article class="relative"><aside class="not-prose absolute -top-[200px] left-full -mb-[100px] hidden h-[calc(100%+150px)] xl:block "><div class="sticky bottom-0  top-[200px] z-10 ml-[5rem] mt-[200px] w-[200px]"><div class="mb-4 border-l px-4 py-2"><div class="mb-1 font-bold">On this page</div><ul class="text-xs"></ul></div><div class="flex gap-2"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-to-line "><path d="M5 3h14"></path><path d="m18 13-6-6-6 6"></path><path d="M12 7v14"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square-text "><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path><path d="M13 8H7"></path><path d="M17 12H7"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2"><span class="sr-only">Copy</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy "><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div></aside><p>자연어 처리(natural language processing)는 인간의 언어 현상을 기계적으로 분석해서 컴퓨터가 이해할 수 있는 형태로 만드는 자연 언어 이해 혹은 그러한 형태를 다시 인간이 이해할 수 있는 언어로 표현하는 제반 기술을 의미한다. (위키피디아)</p>
<p>간단하게 말하면, 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일 이라고 생각하면 될 것 같다.</p>
<h1 id="텍스트">텍스트</h1>
<p>기계학습 모델을 만들기 위해서는 데이터를 모델에 맞게 변형시켜 주어야 한다. 알고리즘에서 텍스트를 그대로 받아들일수 없기 때문에 받아들일 수 있는 어떤 숫자값으로 변환을 해주어야 한다. 하지만 텍스트는 일단 언어가 제각기 다르기 떄문에 텍스트 자체를 어떻게 숫자화 할지 부터 시작해야한다.</p>
<p>그럼 어떤 방법들이 있는지 살펴보자.</p>
<h1 id="bowbag-of-words">BOW(bag of words)</h1>
<p><img src="/posts/ai/nlp-basic/bow-01.png" alt="nlp-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">nlp-1</span></p>
<p>BOW(Bag of words)는 텍스트 데이터를 표현하는 방법 중 하나로 가장 간단하지만 효과적이라 기계학습에서 널리 쓰이는 방법이다. BOW는 텍스트의 구조와 상관없이 단어들을 담는 가방(Bag)으로 생각하면 된다.</p>
<pre><code>The game is fun
The game is interesting
The game is not funny
</code></pre>
<p>세 문장에서 나타나는 단아들을 모으고 세 문장을 각각 binary vector로 표현하는 것이 BOW이다. 각각의 문장을 binary vector로 표현하면 다음과 같다.</p>
<table><thead><tr><th>the</th><th>game</th><th>is</th><th>fun</th><th>interesting</th><th>not</th><th>funny</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr></tbody></table>
<div class="my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary"><div class="callout-contents flex-1">
<p>The game is fun : <strong>[1, 1, 1, 1, 0, 0, 0]</strong></p>
</div></div>
<table><thead><tr><th>the</th><th>game</th><th>is</th><th>fun</th><th>interesting</th><th>not</th><th>funny</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr></tbody></table>
<div class="my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary"><div class="callout-contents flex-1">
<p>The game is interesting : <strong>[1, 1, 1, 0, 1, 0, 0]</strong></p>
</div></div>
<table><thead><tr><th>the</th><th>game</th><th>is</th><th>fun</th><th>interesting</th><th>not</th><th>funny</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr></tbody></table>
<div class="my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary"><div class="callout-contents flex-1">
<p>The game is not funny : <strong>[1, 1, 1, 0, 0, 1, 1]</strong></p>
</div></div>
<p>만약 이 가방에 들어있지 않는 단어가 포함된 문장이 있어도 BOW는 그 없는 단어는 제외하고 있는 단어만을 가지고 벡터로 만들 것이다.</p>
<p>그럼 이 수치로 표현된 값을 어떻게 활용할까?</p>
<p>머신러닝 모델에 입력값으로 사용할 수도 있디만 단순히 계산만으로 문장간의 유사도(Sentence similarity)를 알 수도 있다.</p>
<p><code>the game is fun</code> 과 <code>The game is interesting</code> 문장의 유사도를 구해보자.</p>
<table><thead><tr><th>문장</th><th>벡터값</th></tr></thead><tbody><tr><td>the game is fun</td><td>[<code>1</code>, <code>1</code>, <code>1</code>, 1, 0, 0, 0]</td></tr><tr><td>The game is interesting</td><td>[<code>1</code>, <code>1</code>, <code>1</code>, 0, 1, 0, 0]</td></tr></tbody></table>
<div class="my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary"><div class="callout-contents flex-1">
<p><strong>유사도 = (1x1) + (1x1) + (1x1) + (1x0) + (0x1) + (0x0) + (0x0) = 3</strong></p>
</div></div>
<p>또한 수치로 표한된 값을 이용해 기계학습 모델에 입력값으로 사용할 수가 있다. 문장에 대한 감성분석을 해주는 모델이 있다면, 벡터화된 값을 모델에 입력값으로 사용할 수 있고 모델은 우리가 원하는 <code>Good</code> 또는 <code>Bad</code>의 결과를 출력해 줄 수 있다.</p>
<p><img src="/posts/ai/nlp-basic/bow-ml.png" alt="bow" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">bow</span></p>
<p>하지만 BOW는 몇 가지 단점이 있다.</p>
<ul>
<li><strong>Sparsity</strong></li>
</ul>
<p>실제 사전에는 100만개가 넘는 단어들이 있을 수도 있다. 그렇게 되면 벡터의 차원이 100만개가 넘어가기 때문에 실제 문장하나를 표현할 때 대부분의 값이 0이고 그외의 값들은 상당히 적을 것이다. 결국 학습량이 많아지고 컴퓨터 자원도 상당히 많이 사용하게 된다.<br/>
<!-- -->(the game is fun [1,1,1,1,0,0,0,0,0,0,0,,,,,,0,0,0,0,0])</p>
<ul>
<li><strong>빈번한 단어는 더 많은 힘을 가진다.</strong></li>
</ul>
<p>많이 출현한 단어는 힘이 세진다. 만약 의미없는 단어들이 많이 사용 되었다면 우리가 원하는 결과를 얻기는 어려울 것이다.</p>
<ul>
<li><strong>Out of vocabulary</strong></li>
</ul>
<p>오타, 줄임말 등의 단어들이 포함되면 굉장히 난감해진다.^^;</p>
<ul>
<li><strong>단어의 순서가 무시됨</strong></li>
</ul>
<p>단어의 출현 횟수만 셀수 있고 단어의 순서는 완전히 무시 된다. 단어의 순서가 무시된다는 것은 다른 의미를 가진 문장이 동일한 결과로 해석될 수 있다는 것이다.</p>
<p>전혀 반대의 의미를 가진 두 문장을 보자.</p>
<p><img src="/posts/ai/nlp-basic/bow-02.png" alt="nlp-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">nlp-1</span></p>
<p>두 문장은 의미가 전혀 반대이지만 BOW를 이용해 처리한다면, 동일한 결과를 반환하게 될 것이다.<br/>
<!-- -->이런 단점을 보완하기 위해 좀더 개선된 n-gram이란 것이 있다. BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용하여 어느정도 단어의 순서를 반영 결과에 반영해 준다.</p>
<h1 id="n-gram">N-Gram</h1>
<p>BOW를 조금 더 개선하여 단어 하나만을 보는 것이 아니라 주변의 n개 단어를 뭉쳐서 보는 것이다. 뭉쳐진 n개의 단어들을 gram이라고 한다.<br/>
<!-- -->단어 개수에 따라 부르는 명칭이 다른데 2개의 단어를 묶어서 사용하면 <code>bi-gram</code>, 3개면 <code>tri-gram</code>이라고 부른다.<br/>
<!-- -->(1-gram은 uni-gram이라고 한다.)</p>
<p>다음 문장을 bi-gram를 사용하여 처리 한다면,</p>
<p><strong>&quot;home run&quot;</strong> 과 <strong>&quot;run home&quot;</strong></p>
<div class="my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary"><div class="callout-contents flex-1">
<p>bag of words : [home, run] , [run, home]<br/>
<!-- -->bi-gram : [home run], [run home]</p>
</div></div>
<p>BOW를 사용한다면 두 문장은 같은 백터의 값을 갖게 되겠지만 bi-gram을 사용하면 2개를 뭉쳐서 사용하므로 어느정도의 순서가 보장되는 효과를 볼수 있게 되어 다른 결과 값을 가지게 될 것이다.</p>
<p>이런 특성을 이용해 n-gram은 다음 단어 예측하거나 어떤 단어를 입력 했을때 오타를 발견하고 다른 단어를 추천해 주는데 활용할 수 있다.</p>
<h1 id="텍스트-전처리-preprocessing">텍스트 전처리 (Preprocessing)</h1>
<p><img src="/posts/ai/nlp-basic/process.png" alt="nlp-1" class="mx-auto mb-0 mt-8 rounded-md"/><span class="mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400">nlp-1</span></p>
<p>BOW나 n-gram이나 모두 많이 쓰이지만 가장 중요한 것은 단어의 전처리가 확실해야 한다는 것이다. 이 글에서는 설명을 위해 간단한 문장만을 사용하여 크게 신경을 쓸 필요는 없겠지만, 자연어 처리를 하다보면 다양한 케이스의 문장들을 접하게 될 것이며 이런 문장들을 토큰화하고 불필요한 단어들은 제거하고 같은 의미의 단어들은 치환하는 등의 고단한 작업 들을 해야 할 것이다. 하지만 다행인 것은 이런 전처리 작업들을 편하게 할 수 있도록 도와주는 좋은 라이브러리들이 있다.</p>
<p>다음 포스팅에서는 전처리에 대해 자세히 살펴보도록 하겠다...</p></article><hr/><section></section><div class="group fixed bottom-4 right-4 xl:hidden"><div class="group relative flex flex-col-reverse"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 z-10 transition"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-bolt "><path d="M21 16V8a2 2 0 0 0-1-1.73l-7-4a2 2 0 0 0-2 0l-7 4A2 2 0 0 0 3 8v8a2 2 0 0 0 1 1.73l7 4a2 2 0 0 0 2 0l7-4A2 2 0 0 0 21 16z"></path><circle cx="12" cy="12" r="4"></circle></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 transition"><span class="sr-only">Copy</span><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy "><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 transition"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square-text "><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path><path d="M13 8H7"></path><path d="M17 12H7"></path></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 outline outline-input outline-1 bg-background hover:bg-accent hover:text-accent-foreground aspect-square p-2 absolute bottom-0 right-0 transition"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-to-line "><path d="M5 3h14"></path><path d="m18 13-6-6-6 6"></path><path d="M12 7v14"></path></svg></button></div></div></div></main><footer class="mb-16 mt-20 flex flex-col items-center justify-center gap-4 text-center print:hidden"><div class="flex justify-center gap-4"><a target="_blank" href="https://github.com/"><svg fill="currentColor" viewBox="0 0 16 16" height="30" width="30" class="fill-foreground transition hover:fill-pink-600"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a target="_blank" href="https://www.linkedin.com/in/"><svg viewBox="0 0 24 24" fill="currentColor" height="30" width="30" class="fill-foreground transition hover:fill-pink-600"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div><div>© 2024. <span class="font-semibold">Devtimes</span> all rights reserved.</div></footer><div role="region" aria-label="Notifications (F8)" tabindex="-1" style="pointer-events:none"><ol tabindex="-1" class="fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[300px]"></ol></div><script src="/_next/static/chunks/webpack-5ffb43b53f25eb9b.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/dfa5ebae0ef69c34.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:I[2386,[],\"\"]\n6:I[9261,[],\"\"]\n8:I[6675,[],\"\"]\n9:I[1441,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"332\",\"static/chunks/332-a5eeb5385fe2fce8.js\",\"672\",\"static/chunks/672-63761e12710ab043.js\",\"319\",\"static/chunks/319-a036a883e9a56048.js\",\"185\",\"static/chunks/app/layout-673adf006acb933b.js\"],\"\"]\na:I[8390,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"332\",\"static/chunks/332-a5eeb5385fe2fce8.js\",\"672\",\"static/chun"])</script><script>self.__next_f.push([1,"ks/672-63761e12710ab043.js\",\"319\",\"static/chunks/319-a036a883e9a56048.js\",\"185\",\"static/chunks/app/layout-673adf006acb933b.js\"],\"ThemeProvider\"]\nb:I[7083,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"332\",\"static/chunks/332-a5eeb5385fe2fce8.js\",\"672\",\"static/chunks/672-63761e12710ab043.js\",\"319\",\"static/chunks/319-a036a883e9a56048.js\",\"185\",\"static/chunks/app/layout-673adf006acb933b.js\"],\"Header\"]\nc:I[9605,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\""])</script><script>self.__next_f.push([1,"static/chunks/999-ff03068a6bad5acb.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-0b2148024efdb29d.js\"],\"\"]\nd:I[3760,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"332\",\"static/chunks/332-a5eeb5385fe2fce8.js\",\"672\",\"static/chunks/672-63761e12710ab043.js\",\"319\",\"static/chunks/319-a036a883e9a56048.js\",\"185\",\"static/chunks/app/layout-673adf006acb933b.js\"],\"Toaster\"]\ne:I[9425,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5"])</script><script>self.__next_f.push([1,"acb.js\",\"332\",\"static/chunks/332-a5eeb5385fe2fce8.js\",\"672\",\"static/chunks/672-63761e12710ab043.js\",\"319\",\"static/chunks/319-a036a883e9a56048.js\",\"185\",\"static/chunks/app/layout-673adf006acb933b.js\"],\"GoogleAnalytics\"]\nf:I[1382,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"332\",\"static/chunks/332-a5eeb5385fe2fce8.js\",\"672\",\"static/chunks/672-63761e12710ab043.js\",\"319\",\"static/chunks/319-a036a883e9a56048.js\",\"185\",\"static/chunks/app/layout-673adf006acb933b.js\"]"])</script><script>self.__next_f.push([1,",\"GoogleTagManager\"]\n11:I[5404,[],\"\"]\n7:[\"slug\",\"nlp-basic\",\"d\"]\n12:[]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/dfa5ebae0ef69c34.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"DE6B_eT-bmUXKHLlihOyz\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/nlp-basic\",\"initialTree\":[\"\",{\"children\":[\"(blog)\",{\"children\":[[\"slug\",\"nlp-basic\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"nlp-basic\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"(blog)\",{\"children\":[[\"slug\",\"nlp-basic\",\"d\"],{\"children\":[\"__PAGE__\",{},[\"$L4\",\"$L5\",null]]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blog)\",\"children\",\"$7\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blog)\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"ko\",\"className\":\"h-full scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"$L9\",null,{\"id\":\"adsbygoogle-init\",\"strategy\":\"beforeInteractive\",\"src\":\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1154659137489563\",\"crossOrigin\":\"anonymous\"}]}],[\"$\",\"body\",null,{\"className\":\"flex min-h-screen flex-col font-pretendard\",\"children\":[[\"$\",\"$La\",null,{\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"className\":\"mt-[calc(64px+env(safe-area-inset-top))] flex flex-1 flex-col\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"grid flex-1 place-content-center text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-4 text-2xl font-bold\",\"children\":\"Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-8 text-lg\",\"children\":\"찾을 수 없는 페이지입니다.\"}],[\"$\",\"$Lc\",null,{\"href\":\"/\",\"children\":\"홈으로\",\"className\":\"inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 bg-primary text-primary-foreground hover:bg-primary/90 h-10 py-2 mx-auto w-fit px-10\"}]]}],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"footer\",null,{\"className\":\"mb-16 mt-20 flex flex-col items-center justify-center gap-4 text-center print:hidden\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-4\",\"children\":[[\"$\",\"$Lc\",null,{\"href\":\"https://github.com/\",\"target\":\"_blank\",\"children\":[\"$\",\"svg\",null,{\"fill\":\"currentColor\",\"viewBox\":\"0 0 16 16\",\"height\":30,\"width\":30,\"className\":\"fill-foreground transition hover:fill-pink-600\",\"children\":[\"$\",\"path\",null,{\"d\":\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0016 8c0-4.42-3.58-8-8-8z\"}]}]}],[\"$\",\"$Lc\",null,{\"href\":\"https://www.linkedin.com/in/\",\"target\":\"_blank\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"fill\":\"currentColor\",\"height\":30,\"width\":30,\"className\":\"fill-foreground transition hover:fill-pink-600\",\"children\":[\"$\",\"path\",null,{\"d\":\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"}]}]}]]}],[\"$\",\"div\",null,{\"children\":[\"© 2024. \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Devtimes\"}],\" all rights reserved.\"]}]]}]]}],[\"$\",\"$Ld\",null,{}],[\"$\",\"$Le\",null,{\"gaId\":\"G-4GBTCVBGMX\"}],[\"$\",\"$Lf\",null,{\"gtmId\":\"GTM-M5JW97ZQ\"}]]}]]}],null]],\"initialHead\":[false,\"$L10\"],\"globalErrorComponent\":\"$11\",\"missingSlots\":\"$W12\"}]]\n"])</script><script>self.__next_f.push([1,"13:I[2465,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-0b2148024efdb29d.js\"],\"\"]\n15:I[3574,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-0b2148024efdb29d.js\"],\"\"]\n16:I[938,[\"605\",\"static/chunks/605-9393b79bc12886be.js\",\"999\",\"static/chunks/999-ff03068a6bad5acb.js\",\"206\",\"static/chunks/app/(blog)/%5Bslug%5D/page-0b214802"])</script><script>self.__next_f.push([1,"4efdb29d.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"prose mx-auto w-full max-w-[750px] px-5 dark:prose-invert sm:px-6\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mt-14 text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-5 text-3xl\",\"children\":\"NLP - Bag of words, n-gram\"}],[\"$\",\"div\",null,{\"className\":\"mb-3 text-base\",\"children\":[\"$\",\"$Lc\",null,{\"href\":\"/ai\",\"className\":\"font-semibold text-pink-600 no-underline underline-offset-4 hover:underline\",\"children\":\"Ai\"}]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-3 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-calendar-days w-3.5\",\"children\":[[\"$\",\"path\",\"1cmpym\",{\"d\":\"M8 2v4\"}],[\"$\",\"path\",\"4m81vk\",{\"d\":\"M16 2v4\"}],[\"$\",\"rect\",\"1hopcy\",{\"width\":\"18\",\"height\":\"18\",\"x\":\"3\",\"y\":\"4\",\"rx\":\"2\"}],[\"$\",\"path\",\"8toen8\",{\"d\":\"M3 10h18\"}],[\"$\",\"path\",\"6423bh\",{\"d\":\"M8 14h.01\"}],[\"$\",\"path\",\"1etili\",{\"d\":\"M12 14h.01\"}],[\"$\",\"path\",\"1gbofw\",{\"d\":\"M16 14h.01\"}],[\"$\",\"path\",\"lrp35t\",{\"d\":\"M8 18h.01\"}],[\"$\",\"path\",\"mhygvu\",{\"d\":\"M12 18h.01\"}],[\"$\",\"path\",\"kzsmim\",{\"d\":\"M16 18h.01\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"2019년 08월 07일\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-clock3 w-3.5\",\"children\":[[\"$\",\"circle\",\"1mglay\",{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"polyline\",\"1aq6pp\",{\"points\":\"12 6 12 12 16.5 12\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":[9,\"분\"]}]]}]]}],[\"$\",\"hr\",null,{\"className\":\"mt-5\"}]]}],null,[\"$\",\"article\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"$L13\",null,{\"toc\":[]}],\"$L14\"]}],[\"$\",\"hr\",null,{}],[\"$\",\"$L15\",null,{}],[\"$\",\"$L16\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1, viewport-fit=cover\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"NLP - Bag of words, n-gram | DevTimes\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"NLP - Bag of words, n-gram\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"NLP - Bag of words, n-gram | DevTimes\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"NLP - Bag of words, n-gram\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://devtimes.com/nlp-basic\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://devtimes.com/posts/ai/nlp-basic/cover.png\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"9\",{\"property\":\"article:published_time\",\"content\":\"2019-08-07T00:00:00.000Z\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"NLP - Bag of words, n-gram | DevTimes\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"NLP - Bag of words, n-gram\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:image\",\"content\":\"https://devtimes.com/posts/ai/nlp-basic/cover.png\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"518x518\"}],[\"$\",\"link\",\"15\",{\"rel\":\"icon\",\"href\":\"/icon.png?8d4a794a4e2a4e0b\",\"type\":\"image/png\",\"sizes\":\"518x518\"}],[\"$\",\"link\",\"16\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?8d4a794a4e2a4e0b\",\"type\":\"image/png\",\"sizes\":\"518x518\"}]]\n"])</script><script>self.__next_f.push([1,"4:null\n"])</script><script>self.__next_f.push([1,"14:[[\"$\",\"p\",null,{\"children\":\"자연어 처리(natural language processing)는 인간의 언어 현상을 기계적으로 분석해서 컴퓨터가 이해할 수 있는 형태로 만드는 자연 언어 이해 혹은 그러한 형태를 다시 인간이 이해할 수 있는 언어로 표현하는 제반 기술을 의미한다. (위키피디아)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"간단하게 말하면, 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일 이라고 생각하면 될 것 같다.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"텍스트\",\"children\":\"텍스트\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"기계학습 모델을 만들기 위해서는 데이터를 모델에 맞게 변형시켜 주어야 한다. 알고리즘에서 텍스트를 그대로 받아들일수 없기 때문에 받아들일 수 있는 어떤 숫자값으로 변환을 해주어야 한다. 하지만 텍스트는 일단 언어가 제각기 다르기 떄문에 텍스트 자체를 어떻게 숫자화 할지 부터 시작해야한다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"그럼 어떤 방법들이 있는지 살펴보자.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"bowbag-of-words\",\"children\":\"BOW(bag of words)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/nlp-basic/bow-01.png\",\"alt\":\"nlp-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"nlp-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"BOW(Bag of words)는 텍스트 데이터를 표현하는 방법 중 하나로 가장 간단하지만 효과적이라 기계학습에서 널리 쓰이는 방법이다. BOW는 텍스트의 구조와 상관없이 단어들을 담는 가방(Bag)으로 생각하면 된다.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"The game is fun\\nThe game is interesting\\nThe game is not funny\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"세 문장에서 나타나는 단아들을 모으고 세 문장을 각각 binary vector로 표현하는 것이 BOW이다. 각각의 문장을 binary vector로 표현하면 다음과 같다.\"}],\"\\n\",[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"the\"}],[\"$\",\"th\",null,{\"children\":\"game\"}],[\"$\",\"th\",null,{\"children\":\"is\"}],[\"$\",\"th\",null,{\"children\":\"fun\"}],[\"$\",\"th\",null,{\"children\":\"interesting\"}],[\"$\",\"th\",null,{\"children\":\"not\"}],[\"$\",\"th\",null,{\"children\":\"funny\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"0\"}],[\"$\",\"td\",null,{\"children\":\"0\"}],[\"$\",\"td\",null,{\"children\":\"0\"}]]}]}]]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"callout-contents flex-1\",\"children\":[\"$undefined\",[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The game is fun : \",[\"$\",\"strong\",null,{\"children\":\"[1, 1, 1, 1, 0, 0, 0]\"}]]}],\"\\n\"]]}]]}],\"\\n\",[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"the\"}],[\"$\",\"th\",null,{\"children\":\"game\"}],[\"$\",\"th\",null,{\"children\":\"is\"}],[\"$\",\"th\",null,{\"children\":\"fun\"}],[\"$\",\"th\",null,{\"children\":\"interesting\"}],[\"$\",\"th\",null,{\"children\":\"not\"}],[\"$\",\"th\",null,{\"children\":\"funny\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"0\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"0\"}],[\"$\",\"td\",null,{\"children\":\"0\"}]]}]}]]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"callout-contents flex-1\",\"children\":[\"$undefined\",[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The game is interesting : \",[\"$\",\"strong\",null,{\"children\":\"[1, 1, 1, 0, 1, 0, 0]\"}]]}],\"\\n\"]]}]]}],\"\\n\",[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"the\"}],[\"$\",\"th\",null,{\"children\":\"game\"}],[\"$\",\"th\",null,{\"children\":\"is\"}],[\"$\",\"th\",null,{\"children\":\"fun\"}],[\"$\",\"th\",null,{\"children\":\"interesting\"}],[\"$\",\"th\",null,{\"children\":\"not\"}],[\"$\",\"th\",null,{\"children\":\"funny\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"0\"}],[\"$\",\"td\",null,{\"children\":\"0\"}],[\"$\",\"td\",null,{\"children\":\"1\"}],[\"$\",\"td\",null,{\"children\":\"1\"}]]}]}]]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"callout-contents flex-1\",\"children\":[\"$undefined\",[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The game is not funny : \",[\"$\",\"strong\",null,{\"children\":\"[1, 1, 1, 0, 0, 1, 1]\"}]]}],\"\\n\"]]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"만약 이 가방에 들어있지 않는 단어가 포함된 문장이 있어도 BOW는 그 없는 단어는 제외하고 있는 단어만을 가지고 벡터로 만들 것이다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"그럼 이 수치로 표현된 값을 어떻게 활용할까?\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"머신러닝 모델에 입력값으로 사용할 수도 있디만 단순히 계산만으로 문장간의 유사도(Sentence similarity)를 알 수도 있다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"the game is fun\"}],\" 과 \",[\"$\",\"code\",null,{\"children\":\"The game is interesting\"}],\" 문장의 유사도를 구해보자.\"]}],\"\\n\",[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"문장\"}],[\"$\",\"th\",null,{\"children\":\"벡터값\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"the game is fun\"}],[\"$\",\"td\",null,{\"children\":[\"[\",[\"$\",\"code\",null,{\"children\":\"1\"}],\", \",[\"$\",\"code\",null,{\"children\":\"1\"}],\", \",[\"$\",\"code\",null,{\"children\":\"1\"}],\", 1, 0, 0, 0]\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"The game is interesting\"}],[\"$\",\"td\",null,{\"children\":[\"[\",[\"$\",\"code\",null,{\"children\":\"1\"}],\", \",[\"$\",\"code\",null,{\"children\":\"1\"}],\", \",[\"$\",\"code\",null,{\"children\":\"1\"}],\", 0, 1, 0, 0]\"]}]]}]]}]]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"callout-contents flex-1\",\"children\":[\"$undefined\",[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"유사도 = (1x1) + (1x1) + (1x1) + (1x0) + (0x1) + (0x0) + (0x0) = 3\"}]}],\"\\n\"]]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"또한 수치로 표한된 값을 이용해 기계학습 모델에 입력값으로 사용할 수가 있다. 문장에 대한 감성분석을 해주는 모델이 있다면, 벡터화된 값을 모델에 입력값으로 사용할 수 있고 모델은 우리가 원하는 \",[\"$\",\"code\",null,{\"children\":\"Good\"}],\" 또는 \",[\"$\",\"code\",null,{\"children\":\"Bad\"}],\"의 결과를 출력해 줄 수 있다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/nlp-basic/bow-ml.png\",\"alt\":\"bow\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"bow\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"하지만 BOW는 몇 가지 단점이 있다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Sparsity\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"실제 사전에는 100만개가 넘는 단어들이 있을 수도 있다. 그렇게 되면 벡터의 차원이 100만개가 넘어가기 때문에 실제 문장하나를 표현할 때 대부분의 값이 0이고 그외의 값들은 상당히 적을 것이다. 결국 학습량이 많아지고 컴퓨터 자원도 상당히 많이 사용하게 된다.\",[\"$\",\"br\",null,{}],\"\\n\",\"(the game is fun [1,1,1,1,0,0,0,0,0,0,0,,,,,,0,0,0,0,0])\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"빈번한 단어는 더 많은 힘을 가진다.\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"많이 출현한 단어는 힘이 세진다. 만약 의미없는 단어들이 많이 사용 되었다면 우리가 원하는 결과를 얻기는 어려울 것이다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Out of vocabulary\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"오타, 줄임말 등의 단어들이 포함되면 굉장히 난감해진다.^^;\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"단어의 순서가 무시됨\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"단어의 출현 횟수만 셀수 있고 단어의 순서는 완전히 무시 된다. 단어의 순서가 무시된다는 것은 다른 의미를 가진 문장이 동일한 결과로 해석될 수 있다는 것이다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"전혀 반대의 의미를 가진 두 문장을 보자.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/nlp-basic/bow-02.png\",\"alt\":\"nlp-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"nlp-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"두 문장은 의미가 전혀 반대이지만 BOW를 이용해 처리한다면, 동일한 결과를 반환하게 될 것이다.\",[\"$\",\"br\",null,{}],\"\\n\",\"이런 단점을 보완하기 위해 좀더 개선된 n-gram이란 것이 있다. BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용하여 어느정도 단어의 순서를 반영 결과에 반영해 준다.\"]}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"n-gram\",\"children\":\"N-Gram\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"BOW를 조금 더 개선하여 단어 하나만을 보는 것이 아니라 주변의 n개 단어를 뭉쳐서 보는 것이다. 뭉쳐진 n개의 단어들을 gram이라고 한다.\",[\"$\",\"br\",null,{}],\"\\n\",\"단어 개수에 따라 부르는 명칭이 다른데 2개의 단어를 묶어서 사용하면 \",[\"$\",\"code\",null,{\"children\":\"bi-gram\"}],\", 3개면 \",[\"$\",\"code\",null,{\"children\":\"tri-gram\"}],\"이라고 부른다.\",[\"$\",\"br\",null,{}],\"\\n\",\"(1-gram은 uni-gram이라고 한다.)\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"다음 문장을 bi-gram를 사용하여 처리 한다면,\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"\\\"home run\\\"\"}],\" 과 \",[\"$\",\"strong\",null,{\"children\":\"\\\"run home\\\"\"}]]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"my-6 flex items-center gap-3 rounded-md px-5 py-4 text-secondary-foreground bg-secondary\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"callout-contents flex-1\",\"children\":[\"$undefined\",[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"bag of words : [home, run] , [run, home]\",[\"$\",\"br\",null,{}],\"\\n\",\"bi-gram : [home run], [run home]\"]}],\"\\n\"]]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"BOW를 사용한다면 두 문장은 같은 백터의 값을 갖게 되겠지만 bi-gram을 사용하면 2개를 뭉쳐서 사용하므로 어느정도의 순서가 보장되는 효과를 볼수 있게 되어 다른 결과 값을 가지게 될 것이다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이런 특성을 이용해 n-gram은 다음 단어 예측하거나 어떤 단어를 입력 했을때 오타를 발견하고 다른 단어를 추천해 주는데 활용할 수 있다.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"텍스트-전처리-preprocessing\",\"children\":\"텍스트 전처리 (Preprocessing)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"img\",null,{\"src\":\"/posts/ai/nlp-basic/process.png\",\"alt\":\"nlp-1\",\"className\":\"mx-auto mb-0 mt-8 rounded-md\"}],[\"$\",\"span\",null,{\"className\":\"mb-8 mt-2 block w-full text-center text-sm text-gray-500 dark:text-gray-400\",\"children\":\"nlp-1\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"BOW나 n-gram이나 모두 많이 쓰이지만 가장 중요한 것은 단어의 전처리가 확실해야 한다는 것이다. 이 글에서는 설명을 위해 간단한 문장만을 사용하여 크게 신경을 쓸 필요는 없겠지만, 자연어 처리를 하다보면 다양한 케이스의 문장들을 접하게 될 것이며 이런 문장들을 토큰화하고 불필요한 단어들은 제거하고 같은 의미의 단어들은 치환하는 등의 고단한 작업 들을 해야 할 것이다. 하지만 다행인 것은 이런 전처리 작업들을 편하게 할 수 있도록 도와주는 좋은 라이브러리들이 있다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"다음 포스팅에서는 전처리에 대해 자세히 살펴보도록 하겠다...\"}]]\n"])</script><script>self.__next_f.push([1,""])</script></body></html>